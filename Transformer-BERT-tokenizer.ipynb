{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport random\n\n#replace with pytorch lightning\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n\nfrom sklearn.preprocessing import MinMaxScaler    \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport string","metadata":{"execution":{"iopub.status.busy":"2022-11-22T18:39:44.258147Z","iopub.execute_input":"2022-11-22T18:39:44.258645Z","iopub.status.idle":"2022-11-22T18:39:44.267803Z","shell.execute_reply.started":"2022-11-22T18:39:44.258613Z","shell.execute_reply":"2022-11-22T18:39:44.264913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice = torch.device(device)\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-11-22T18:39:45.753674Z","iopub.execute_input":"2022-11-22T18:39:45.754186Z","iopub.status.idle":"2022-11-22T18:39:45.897397Z","shell.execute_reply.started":"2022-11-22T18:39:45.754147Z","shell.execute_reply":"2022-11-22T18:39:45.896311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n    \"\"\"\n    Mask the upper half of the dot product matrix in self attention.\n    This prevents flow of information from future tokens to current token.\n    1's in the lower triangle, counting from the lower right corner.\n    \"\"\"\n    i=torch.range(1,n_dest)[:,None]\n    j=torch.range(1,n_src)\n    m = i >= j - n_src + n_dest\n    mask=m.bool()\n    return ~mask\n#     mask=torch.reshape(mask, [1, n_dest, n_src])\n#     mult=[batch_size,1,1]\n#     return torch.tile(mask,mult);","metadata":{"execution":{"iopub.status.busy":"2022-11-22T18:39:47.896101Z","iopub.execute_input":"2022-11-22T18:39:47.896459Z","iopub.status.idle":"2022-11-22T18:39:47.903313Z","shell.execute_reply.started":"2022-11-22T18:39:47.896430Z","shell.execute_reply":"2022-11-22T18:39:47.902164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"causal_attention_mask(2,5,5,torch.bool)","metadata":{"execution":{"iopub.status.busy":"2022-11-22T18:39:50.090784Z","iopub.execute_input":"2022-11-22T18:39:50.091457Z","iopub.status.idle":"2022-11-22T18:39:50.105541Z","shell.execute_reply.started":"2022-11-22T18:39:50.091423Z","shell.execute_reply":"2022-11-22T18:39:50.104466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def padding_mask(input):\n    # Create mask which marks the zero padding values in the input by a 1\n#     print(input)\n#     input=torch.tensor(input['train']['input_ids'])\n    mask=torch.eq(input, torch.zeros_like(input))\n\n \n    return mask","metadata":{"execution":{"iopub.status.busy":"2022-11-22T18:39:50.752838Z","iopub.execute_input":"2022-11-22T18:39:50.753228Z","iopub.status.idle":"2022-11-22T18:39:50.760794Z","shell.execute_reply.started":"2022-11-22T18:39:50.753197Z","shell.execute_reply":"2022-11-22T18:39:50.759865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"padding_mask(torch.tensor([[1,2,3,0,0,0],[2,0,0,0,0,0]]))","metadata":{"execution":{"iopub.status.busy":"2022-11-22T18:39:52.770921Z","iopub.execute_input":"2022-11-22T18:39:52.771926Z","iopub.status.idle":"2022-11-22T18:39:52.782883Z","shell.execute_reply.started":"2022-11-22T18:39:52.771884Z","shell.execute_reply":"2022-11-22T18:39:52.781764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(self, embed_dim, num_heads,batch_first,rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.attention = nn.MultiheadAttention(embed_dim,num_heads,batch_first=batch_first)\n\n    def forward(self, inputs,pad_mask):\n        input_shape = inputs.size()\n        batch_size = input_shape[0]\n        seq_len = input_shape[1]\n#         pad_mask=padding_mask(inputs)\n        pad_mask.to(device)\n        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, torch.bool).to(device)\n        attention_output,a = self.attention(inputs, inputs,inputs, key_padding_mask=pad_mask, attn_mask=causal_mask)\n        return attention_output","metadata":{"execution":{"iopub.status.busy":"2022-11-22T19:05:20.678947Z","iopub.execute_input":"2022-11-22T19:05:20.679310Z","iopub.status.idle":"2022-11-22T19:05:20.686557Z","shell.execute_reply.started":"2022-11-22T19:05:20.679279Z","shell.execute_reply":"2022-11-22T19:05:20.685426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TokenAndPositionEmbedding(nn.Module):\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        super(TokenAndPositionEmbedding, self).__init__()\n        self.token_emb = nn.Embedding(vocab_size, embed_dim,max_norm=1)\n        self.pos_emb = nn.Embedding(maxlen,embed_dim)\n\n    def forward(self, x):\n        maxlen = x.size()[-1]\n        pad_mask=padding_mask(x)\n        positions = torch.range(start=0, end=maxlen-1, step=1,dtype=torch.int32).to(device)\n        positions = self.pos_emb(positions)\n        x = self.token_emb(x)\n        return (x + positions),pad_mask","metadata":{"execution":{"iopub.status.busy":"2022-11-22T19:05:22.475877Z","iopub.execute_input":"2022-11-22T19:05:22.476236Z","iopub.status.idle":"2022-11-22T19:05:22.483679Z","shell.execute_reply.started":"2022-11-22T19:05:22.476207Z","shell.execute_reply":"2022-11-22T19:05:22.482372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size = 28996  # Only consider the top 20k words\nmaxlen = 80  # Max sequence size\nembed_dim = 256  # Embedding size for each token\nnum_heads = 8  # Number of attention heads\nfeed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n        self.transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim,True)\n        self.outputs= nn.LazyLinear(vocab_size)\n        \n    def forward(self, x):\n        x,pad_mask = self.embedding_layer(x)\n#         print(x,\"maximum=\",torch.max(x))\n        x = self.transformer_block(x,pad_mask)\n#         print(x)\n        x = self.outputs(x)\n        \n        return x\n","metadata":{"execution":{"iopub.status.busy":"2022-11-22T19:05:23.944625Z","iopub.execute_input":"2022-11-22T19:05:23.945292Z","iopub.status.idle":"2022-11-22T19:05:23.952362Z","shell.execute_reply.started":"2022-11-22T19:05:23.945255Z","shell.execute_reply":"2022-11-22T19:05:23.951283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport re\ndirectories = [\n    \"/kaggle/input/aclimdb-v1/aclImdb/train/pos\",\n    \"/kaggle/input/aclimdb-v1/aclImdb/train/neg\",\n    \"/kaggle/input/aclimdb-v1/aclImdb/test/pos\",\n    \"/kaggle/input/aclimdb-v1/aclImdb/test/neg\",\n]\n\nfrom datasets import load_dataset\nfilenames = []\nfor dir in directories:\n    for f in os.listdir(dir):\n        filenames.append(os.path.join(dir, f))\n\ndataset = load_dataset(\"text\", data_files=filenames)\n\ndef processing(s):\n  s['text']=s['text'].lower()\n  s['text']=re.sub(\"<br />\", \" \", s['text'])\n  s['text']=re.sub(f\"([{string.punctuation}])\", r\" \\1\", s['text'])\n  return s\n\ndataset=dataset.map(processing)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-11-22T18:40:02.533415Z","iopub.execute_input":"2022-11-22T18:40:02.533801Z","iopub.status.idle":"2022-11-22T18:47:42.284658Z","shell.execute_reply.started":"2022-11-22T18:40:02.533770Z","shell.execute_reply":"2022-11-22T18:47:42.283652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")","metadata":{"execution":{"iopub.status.busy":"2022-11-22T18:47:42.286695Z","iopub.execute_input":"2022-11-22T18:47:42.287353Z","iopub.status.idle":"2022-11-22T18:47:45.247584Z","shell.execute_reply.started":"2022-11-22T18:47:42.287315Z","shell.execute_reply":"2022-11-22T18:47:45.246639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = dataset.map(lambda dataset: tokenizer(dataset[\"text\"], padding=\"max_length\",truncation=True, max_length=80))","metadata":{"execution":{"iopub.status.busy":"2022-11-22T18:47:59.948561Z","iopub.execute_input":"2022-11-22T18:47:59.948966Z","iopub.status.idle":"2022-11-22T18:49:03.545137Z","shell.execute_reply.started":"2022-11-22T18:47:59.948933Z","shell.execute_reply":"2022-11-22T18:49:03.544176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2022-11-21T00:51:56.809879Z","iopub.execute_input":"2022-11-21T00:51:56.810323Z","iopub.status.idle":"2022-11-21T00:51:56.818975Z","shell.execute_reply.started":"2022-11-21T00:51:56.810286Z","shell.execute_reply":"2022-11-21T00:51:56.817928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model= Model()\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-11-22T19:05:26.987618Z","iopub.execute_input":"2022-11-22T19:05:26.987983Z","iopub.status.idle":"2022-11-22T19:05:27.087872Z","shell.execute_reply.started":"2022-11-22T19:05:26.987953Z","shell.execute_reply":"2022-11-22T19:05:27.086774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TextGenerator(nn.Module):\n    \"\"\"A callback to generate text from a trained model.\n    1. Feed some starting prompt to the model\n    2. Predict probabilities for the next token\n    3. Sample the next token and add it to the next input\n\n    Arguments:\n        max_tokens: Integer, the number of tokens to be generated after prompt.\n        start_tokens: List of integers, the token indices for the starting prompt.\n        index_to_word: List of strings, obtained from the TextVectorization layer.\n        top_k: Integer, sample from the `top_k` token predictions.\n        print_every: Integer, print after this many epochs.\n    \"\"\"\n\n    def __init__(\n        self, max_tokens, start_tokens, top_k=10, print_every=1\n    ):\n        self.max_tokens = max_tokens\n        self.start_tokens = start_tokens\n#         self.index_to_word = index_to_word\n        self.print_every = print_every\n        self.k = top_k\n\n    def sample_from(self, logits):\n        logits, indices = torch.topk(logits, k=self.k, sorted=True)\n        logits=logits.cpu()\n        indices=indices.cpu()\n        indices = np.asarray(indices).astype(\"int32\")\n       \n        softmax=nn.Softmax(dim=0)\n        preds = softmax(logits)\n        preds = np.asarray(preds).astype(\"float32\")\n#         return np.random.choice(indices, p=preds) THIS IS THE CORRECT CODE, BUT HAD TO COMMENT IT AS\n#.        PROBABILITIES HAVE NAN AND I HAD TO VERIFY PIPELINE, BELOW LINE WILL BE REMOVED ONCE NAN ISSUE \n#.        IS RESOLVED\n        return np.random.choice(indices, p=preds)\n#         return np.random.choice(5, 1, p=[0.1, 0, 0.3, 0.6, 0])\n\n    def detokenize(self, number):\n        return tokenizer.decode(number)\n\n    def on_epoch_end(self, epoch, logs=None):\n        start_tokens = [_ for _ in self.start_tokens]\n        if (epoch + 1) % self.print_every != 0:\n            return\n        num_tokens_generated = 0\n        tokens_generated = []\n        while num_tokens_generated <= self.max_tokens:\n            pad_len = maxlen - len(start_tokens)\n            sample_index = len(start_tokens) - 1\n            if pad_len < 0:\n                data = start_tokens[:maxlen]\n                sample_index = maxlen - 1\n            elif pad_len > 0:\n                data = start_tokens + [0] * pad_len\n            else:\n                data = start_tokens\n                \n            data = torch.Tensor(np.array([data])).type(torch.int32).to(device)\n            \n            y = model(data)\n            \n            sample_token = self.sample_from(y[0][sample_index])\n            tokens_generated.append(sample_token)\n            start_tokens.append(sample_token)\n            num_tokens_generated = len(tokens_generated)\n        txt = \" \".join(\n            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n        )\n        print(f\"generated text:\\n{txt}\\n\")\n\n\n# Tokenize starting prompt\n# word_to_index = {}\n# for index, word in enumerate(vocab):\n#     word_to_index[word] = index\n\nstart_prompt = \"this movie is\"\nstart_tokens=tokenizer(start_prompt)['input_ids']\n# start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\nnum_tokens_generated = 40\n# text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)","metadata":{"execution":{"iopub.status.busy":"2022-11-22T19:15:58.347930Z","iopub.execute_input":"2022-11-22T19:15:58.348725Z","iopub.status.idle":"2022-11-22T19:15:58.363266Z","shell.execute_reply.started":"2022-11-22T19:15:58.348680Z","shell.execute_reply":"2022-11-22T19:15:58.362133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\ntrain_dataset= Dataset.from_dict({\"id\": dataset['train']['input_ids']})\ntrain_dataset = train_dataset.with_format(\"torch\")","metadata":{"execution":{"iopub.status.busy":"2022-11-22T18:49:21.251587Z","iopub.execute_input":"2022-11-22T18:49:21.251948Z","iopub.status.idle":"2022-11-22T18:49:23.292513Z","shell.execute_reply.started":"2022-11-22T18:49:21.251917Z","shell.execute_reply":"2022-11-22T18:49:23.291553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader=DataLoader(train_dataset,batch_size=50,shuffle=True)\noptim=torch.optim.AdamW(model.parameters(),lr=5e-5)\nloss_fn=torch.nn.CrossEntropyLoss()\ncount=0\nfor epoch in tqdm(range(30)):\n    for batch in tqdm(train_loader):\n        optim.zero_grad()\n#         print(batch['id'][:,:-1])\n        input_ids=batch['id'][:,:-1].to(device)\n        labels=batch['id'][:,1:].to(device)\n        outputs=model.forward(input_ids)\n        labels=nn.functional.one_hot(labels,num_classes=vocab_size).type(torch.float)\n        loss=loss_fn(outputs,labels)\n        loss.backward()\n        optim.step()\n    \n    with torch.no_grad():\n        TextGenerator(40, start_tokens).on_epoch_end(epoch);\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2022-11-22T19:16:13.420011Z","iopub.execute_input":"2022-11-22T19:16:13.420368Z","iopub.status.idle":"2022-11-22T19:21:33.887749Z","shell.execute_reply.started":"2022-11-22T19:16:13.420339Z","shell.execute_reply":"2022-11-22T19:21:33.886766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir /kaggle/working/saved_models/\ntorch.save(model.state_dict,\"/kaggle/working/saved_models/transformer_weights.pth\")","metadata":{},"execution_count":null,"outputs":[]}]}