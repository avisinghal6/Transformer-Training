{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport random\n\n#replace with pytorch lightning\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n\nfrom sklearn.preprocessing import MinMaxScaler    \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport string","metadata":{"execution":{"iopub.status.busy":"2022-11-28T21:36:33.192889Z","iopub.execute_input":"2022-11-28T21:36:33.193219Z","iopub.status.idle":"2022-11-28T21:36:36.102488Z","shell.execute_reply.started":"2022-11-28T21:36:33.193128Z","shell.execute_reply":"2022-11-28T21:36:36.101118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice = torch.device(device)\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-11-28T21:37:11.024289Z","iopub.execute_input":"2022-11-28T21:37:11.024824Z","iopub.status.idle":"2022-11-28T21:37:11.093422Z","shell.execute_reply.started":"2022-11-28T21:37:11.024785Z","shell.execute_reply":"2022-11-28T21:37:11.092215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n    \"\"\"\n    Mask the upper half of the dot product matrix in self attention.\n    This prevents flow of information from future tokens to current token.\n    1's in the lower triangle, counting from the lower right corner.\n    \"\"\"\n    i=torch.range(1,n_dest)[:,None]\n    j=torch.range(1,n_src)\n    m = i >= j - n_src + n_dest\n    mask=m.bool()\n    return ~mask\n#     mask=torch.reshape(mask, [1, n_dest, n_src])\n#     mult=[batch_size,1,1]\n#     return torch.tile(mask,mult);","metadata":{"execution":{"iopub.status.busy":"2022-11-28T21:37:11.684847Z","iopub.execute_input":"2022-11-28T21:37:11.685371Z","iopub.status.idle":"2022-11-28T21:37:11.692520Z","shell.execute_reply.started":"2022-11-28T21:37:11.685328Z","shell.execute_reply":"2022-11-28T21:37:11.691384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"causal_attention_mask(2,5,5,torch.bool)","metadata":{"execution":{"iopub.status.busy":"2022-11-28T21:37:12.973393Z","iopub.execute_input":"2022-11-28T21:37:12.973754Z","iopub.status.idle":"2022-11-28T21:37:12.987615Z","shell.execute_reply.started":"2022-11-28T21:37:12.973721Z","shell.execute_reply":"2022-11-28T21:37:12.986627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def padding_mask(input):\n    # Create mask which marks the zero padding values in the input by a 1\n#     print(input)\n#     input=torch.tensor(input['train']['input_ids'])\n    mask=torch.eq(input, torch.zeros_like(input))\n\n \n    return mask","metadata":{"execution":{"iopub.status.busy":"2022-11-28T21:37:13.474200Z","iopub.execute_input":"2022-11-28T21:37:13.475325Z","iopub.status.idle":"2022-11-28T21:37:13.481010Z","shell.execute_reply.started":"2022-11-28T21:37:13.475282Z","shell.execute_reply":"2022-11-28T21:37:13.479671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"padding_mask(torch.tensor([[1,2,3,0,0,0],[2,0,0,0,0,0]]))","metadata":{"execution":{"iopub.status.busy":"2022-11-28T21:37:14.657314Z","iopub.execute_input":"2022-11-28T21:37:14.659449Z","iopub.status.idle":"2022-11-28T21:37:14.668872Z","shell.execute_reply.started":"2022-11-28T21:37:14.659401Z","shell.execute_reply":"2022-11-28T21:37:14.667799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(self, embed_dim, num_heads,batch_first,rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.attention = nn.MultiheadAttention(embed_dim,num_heads,batch_first=batch_first)\n\n    def forward(self, inputs,pad_mask):\n        input_shape = inputs.size()\n        batch_size = input_shape[0]\n        seq_len = input_shape[1]\n#         pad_mask=padding_mask(inputs)\n        pad_mask.to(device)\n        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, torch.bool).to(device)\n        attention_output,a = self.attention(inputs, inputs,inputs, key_padding_mask=pad_mask, attn_mask=causal_mask,need_weights=True,average_attn_weights=False)\n        return attention_output,a","metadata":{"execution":{"iopub.status.busy":"2022-11-28T21:37:15.164979Z","iopub.execute_input":"2022-11-28T21:37:15.165696Z","iopub.status.idle":"2022-11-28T21:37:15.173383Z","shell.execute_reply.started":"2022-11-28T21:37:15.165658Z","shell.execute_reply":"2022-11-28T21:37:15.172018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TokenAndPositionEmbedding(nn.Module):\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        super(TokenAndPositionEmbedding, self).__init__()\n        self.token_emb = nn.Embedding(vocab_size, embed_dim,max_norm=1)\n        self.pos_emb = nn.Embedding(maxlen,embed_dim)\n\n    def forward(self, x):\n        maxlen = x.size()[-1]\n        pad_mask=padding_mask(x)\n        positions = torch.range(start=0, end=maxlen-1, step=1,dtype=torch.int32).to(device)\n        positions = self.pos_emb(positions)\n        x = self.token_emb(x)\n        return (x + positions),pad_mask","metadata":{"execution":{"iopub.status.busy":"2022-11-28T21:37:16.354669Z","iopub.execute_input":"2022-11-28T21:37:16.355048Z","iopub.status.idle":"2022-11-28T21:37:16.362743Z","shell.execute_reply.started":"2022-11-28T21:37:16.355014Z","shell.execute_reply":"2022-11-28T21:37:16.361487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size =50257 #28996  # Only consider the top 20k words\nmaxlen = 20  # Max sequence size\nembed_dim = 128  # Embedding size for each token\nnum_heads = 8  # Number of attention heads\nfeed_forward_dim = 128  # Hidden layer size in feed forward network inside transformer\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n        self.transformer_block1 = TransformerBlock(embed_dim, num_heads,True)\n        self.transformer_block2 = TransformerBlock(embed_dim, num_heads,True)\n        self.MLP1=nn.LazyLinear(feed_forward_dim)\n        self.MLP2=nn.LazyLinear(feed_forward_dim)\n        self.outputs= nn.LazyLinear(vocab_size)\n        \n    def forward(self, x):\n        x,pad_mask = self.embedding_layer(x)\n#         print(x,\"maximum=\",torch.max(x))\n        x,a = self.transformer_block1(x,pad_mask)\n        x=self.MLP1(x);\n        x,a = self.transformer_block2(x,pad_mask)\n#         print(x)\n        x=self.MLP2(x);\n        x = self.outputs(x)\n        \n        return x,a\n","metadata":{"execution":{"iopub.status.busy":"2022-11-28T21:37:16.916953Z","iopub.execute_input":"2022-11-28T21:37:16.917332Z","iopub.status.idle":"2022-11-28T21:37:16.925660Z","shell.execute_reply.started":"2022-11-28T21:37:16.917297Z","shell.execute_reply":"2022-11-28T21:37:16.924348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport re\ndirectories = [\n    \"/kaggle/input/aclimdb-v1/aclImdb/train/pos\",\n    \"/kaggle/input/aclimdb-v1/aclImdb/train/neg\",\n    \"/kaggle/input/aclimdb-v1/aclImdb/test/pos\",\n    \"/kaggle/input/aclimdb-v1/aclImdb/test/neg\",\n]\n\nfrom datasets import load_dataset\nfilenames = []\nfor dir in directories:\n    for f in os.listdir(dir):\n        filenames.append(os.path.join(dir, f))\n\ndataset = load_dataset(\"text\", data_files=filenames)\n\ndef processing(s):\n  s['text']=s['text'].lower()\n  s['text']=re.sub(\"<br />\", \" \", s['text'])\n  s['text']=re.sub(f\"([{string.punctuation}])\", r\" \\1\", s['text'])\n  return s\n\ndataset=dataset.map(processing)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-11-24T17:49:16.259722Z","iopub.execute_input":"2022-11-24T17:49:16.260411Z","iopub.status.idle":"2022-11-24T18:02:03.996988Z","shell.execute_reply.started":"2022-11-24T17:49:16.260376Z","shell.execute_reply":"2022-11-24T18:02:03.995999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers import AutoTokenizer\n# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\nfrom transformers import GPT2Tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")","metadata":{"execution":{"iopub.status.busy":"2022-11-28T21:37:20.318824Z","iopub.execute_input":"2022-11-28T21:37:20.319379Z","iopub.status.idle":"2022-11-28T21:37:24.751176Z","shell.execute_reply.started":"2022-11-28T21:37:20.319330Z","shell.execute_reply":"2022-11-28T21:37:24.750051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = dataset.map(lambda dataset: tokenizer(dataset[\"text\"],truncation=True, max_length=maxlen))","metadata":{"execution":{"iopub.status.busy":"2022-11-28T21:37:33.452063Z","iopub.execute_input":"2022-11-28T21:37:33.452767Z","iopub.status.idle":"2022-11-28T21:37:33.682385Z","shell.execute_reply.started":"2022-11-28T21:37:33.452729Z","shell.execute_reply":"2022-11-28T21:37:33.680920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def padding(s):\n    if len(s['input_ids'])<maxlen :\n        s['input_ids']=s['input_ids']+[0]*(maxlen-len(s['input_ids']))\n    return s\n                                           \ndataset=dataset.map(padding)\n","metadata":{"execution":{"iopub.status.busy":"2022-11-24T18:13:55.808220Z","iopub.execute_input":"2022-11-24T18:13:55.808595Z","iopub.status.idle":"2022-11-24T18:14:04.576526Z","shell.execute_reply.started":"2022-11-24T18:13:55.808565Z","shell.execute_reply":"2022-11-24T18:14:04.575484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2022-11-23T19:37:34.968095Z","iopub.execute_input":"2022-11-23T19:37:34.968708Z","iopub.status.idle":"2022-11-23T19:37:34.976015Z","shell.execute_reply.started":"2022-11-23T19:37:34.968669Z","shell.execute_reply":"2022-11-23T19:37:34.974984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=Model()\n# model.load_state_dict(torch.load(\"/kaggle/input/weights3/transformer_weights-3.pth\")) \nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-11-28T21:38:26.027418Z","iopub.execute_input":"2022-11-28T21:38:26.027802Z","iopub.status.idle":"2022-11-28T21:38:30.663187Z","shell.execute_reply.started":"2022-11-28T21:38:26.027769Z","shell.execute_reply":"2022-11-28T21:38:30.662089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<!-- # model= Model()\n# model.to(device) -->","metadata":{"execution":{"iopub.status.busy":"2022-11-23T19:37:34.977324Z","iopub.execute_input":"2022-11-23T19:37:34.977741Z","iopub.status.idle":"2022-11-23T19:37:35.372676Z","shell.execute_reply.started":"2022-11-23T19:37:34.977705Z","shell.execute_reply":"2022-11-23T19:37:35.371685Z"}}},{"cell_type":"code","source":"class TextGenerator(nn.Module):\n    \"\"\"A callback to generate text from a trained model.\n    1. Feed some starting prompt to the model\n    2. Predict probabilities for the next token\n    3. Sample the next token and add it to the next input\n\n    Arguments:\n        max_tokens: Integer, the number of tokens to be generated after prompt.\n        start_tokens: List of integers, the token indices for the starting prompt.\n        index_to_word: List of strings, obtained from the TextVectorization layer.\n        top_k: Integer, sample from the `top_k` token predictions.\n        print_every: Integer, print after this many epochs.\n    \"\"\"\n\n    def __init__(\n        self, max_tokens, start_tokens, top_k=10, print_every=1\n    ):\n        self.max_tokens = max_tokens\n        self.start_tokens = start_tokens\n#         self.index_to_word = index_to_word\n        self.print_every = print_every\n        self.k = top_k\n\n    def sample_from(self, logits):\n        logits, indices = torch.topk(logits, k=self.k, sorted=True)\n        logits=logits.cpu()\n        indices=indices.cpu()\n        indices = np.asarray(indices).astype(\"int32\")\n       \n        softmax=nn.Softmax(dim=0)\n        preds = softmax(logits)\n        preds = np.asarray(preds).astype(\"float32\")\n#         return np.random.choice(indices, p=preds) THIS IS THE CORRECT CODE, BUT HAD TO COMMENT IT AS\n#.        PROBABILITIES HAVE NAN AND I HAD TO VERIFY PIPELINE, BELOW LINE WILL BE REMOVED ONCE NAN ISSUE \n#.        IS RESOLVED\n        return np.random.choice(indices, p=preds)\n#         return np.random.choice(5, 1, p=[0.1, 0, 0.3, 0.6, 0])\n\n    def detokenize(self, number):\n        return tokenizer.decode(number)\n\n    def on_epoch_end(self, epoch, logs=None):\n        start_tokens = [_ for _ in self.start_tokens]\n        if (epoch + 1) % self.print_every != 0:\n            return\n        num_tokens_generated = 0\n        tokens_generated = []\n        attention_scores=[]\n        while num_tokens_generated <= self.max_tokens:\n            pad_len = maxlen - len(start_tokens)\n            sample_index = len(start_tokens) - 1\n            if pad_len < 0:\n                data = start_tokens[:maxlen]\n                sample_index = maxlen - 1\n            elif pad_len > 0:\n                data = start_tokens + [0] * pad_len\n            else:\n                data = start_tokens\n                \n            data = torch.Tensor(np.array([data])).type(torch.int32).to(device)\n            \n            y,attention_scores = model(data)\n            sample_token = self.sample_from(y[0][sample_index])\n            tokens_generated.append(sample_token)\n            start_tokens.append(sample_token)\n            num_tokens_generated = len(tokens_generated)\n        txt = \" \".join(\n            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n        )\n        print(f\"generated text:\\n{txt}\\n\")\n        return attention_scores\n\n# Tokenize starting prompt\n# word_to_index = {}\n# for index, word in enumerate(vocab):\n#     word_to_index[word] = index\n\nstart_prompt = \"the movie is good but it could have been better, the actors were\"\nstart_tokens=tokenizer(start_prompt)['input_ids']\n# start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\nnum_tokens_generated = 40\n# text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)","metadata":{"execution":{"iopub.status.busy":"2022-11-28T21:38:51.077065Z","iopub.execute_input":"2022-11-28T21:38:51.077447Z","iopub.status.idle":"2022-11-28T21:38:51.095103Z","shell.execute_reply.started":"2022-11-28T21:38:51.077415Z","shell.execute_reply":"2022-11-28T21:38:51.094208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\ntrain_dataset= Dataset.from_dict({\"id\": dataset['train']['input_ids']})\ntrain_dataset = train_dataset.with_format(\"torch\")","metadata":{"execution":{"iopub.status.busy":"2022-11-23T19:37:35.390463Z","iopub.execute_input":"2022-11-23T19:37:35.391040Z","iopub.status.idle":"2022-11-23T19:37:37.379199Z","shell.execute_reply.started":"2022-11-23T19:37:35.391003Z","shell.execute_reply":"2022-11-23T19:37:37.377983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count=0\nTEST=[]\ntrain_loader=DataLoader(train_dataset,batch_size=50,shuffle=True)\nfor i in train_loader:\n    TEST.append(random.choice(i['id']))\n    if count>100:\n        break\n    count+=1","metadata":{"execution":{"iopub.status.busy":"2022-11-23T19:37:37.383762Z","iopub.execute_input":"2022-11-23T19:37:37.384709Z","iopub.status.idle":"2022-11-23T19:37:38.360814Z","shell.execute_reply.started":"2022-11-23T19:37:37.384670Z","shell.execute_reply":"2022-11-23T19:37:38.359756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TEST=torch.stack(TEST,0) \n# print(TEST[:,:])","metadata":{"execution":{"iopub.status.busy":"2022-11-23T19:37:38.362369Z","iopub.execute_input":"2022-11-23T19:37:38.362735Z","iopub.status.idle":"2022-11-23T19:37:38.367810Z","shell.execute_reply.started":"2022-11-23T19:37:38.362697Z","shell.execute_reply":"2022-11-23T19:37:38.366821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader=DataLoader(train_dataset,batch_size=50,shuffle=True)\noptim=torch.optim.AdamW(model.parameters(),lr=1e-4)\nloss_fn=torch.nn.CrossEntropyLoss()\ncount=0\nloss_stats = {\n    'test': [],\n}\nfor epoch in tqdm(range(20)):\n    for batch in tqdm(train_loader):\n        optim.zero_grad()\n#         print(batch['id'][:,:-1])\n        input_ids=batch['id'][:,:-1].to(device)\n#         print(input_ids.shape())\n        labels=batch['id'][:,1:].to(device)\n        outputs,attention_scores=model.forward(input_ids)\n        labels=nn.functional.one_hot(labels,num_classes=vocab_size).type(torch.float)\n        loss=loss_fn(outputs,labels)\n        loss.backward()\n        optim.step()\n    \n    with torch.no_grad():\n        TextGenerator(40, start_tokens).on_epoch_end(epoch);\n        test_input=TEST[:,:-1].to(device)\n        test_output=TEST[:,1:].to(device)\n        outputs,attention_scores=model.forward(test_input)\n        labels=nn.functional.one_hot(test_output,num_classes=vocab_size).type(torch.float)\n        loss=loss_fn(outputs,labels).cpu().item()\n        loss_stats['test'].append(loss)\n        ","metadata":{"execution":{"iopub.status.busy":"2022-11-23T20:02:26.594334Z","iopub.execute_input":"2022-11-23T20:02:26.594727Z","iopub.status.idle":"2022-11-23T20:05:58.721767Z","shell.execute_reply.started":"2022-11-23T20:02:26.594694Z","shell.execute_reply":"2022-11-23T20:05:58.720631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loss_df = pd.DataFrame.from_dict(loss_stats).reset_index().melt(id_vars=['index']).rename(columns={\"index\":\"epochs\"})\n# Plot the dataframes\nfig,axes = plt.subplots(nrows=1, ncols=1, figsize=(20,7))\nsns.lineplot(data=test_loss_df, x = \"epochs\", y=\"value\", hue=\"variable\",  ax=axes).set_title('TestLoss')\n","metadata":{"execution":{"iopub.status.busy":"2022-11-23T20:06:09.792378Z","iopub.execute_input":"2022-11-23T20:06:09.793293Z","iopub.status.idle":"2022-11-23T20:06:10.077278Z","shell.execute_reply.started":"2022-11-23T20:06:09.793257Z","shell.execute_reply":"2022-11-23T20:06:10.076327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir /kaggle/working/saved_models/\ntorch.save(model.state_dict(),\"/kaggle/working/saved_models/transformer_weights.pth\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    score=TextGenerator(6, start_tokens).on_epoch_end(1);\nscore=score.cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2022-11-28T21:39:20.965958Z","iopub.execute_input":"2022-11-28T21:39:20.966572Z","iopub.status.idle":"2022-11-28T21:39:29.882384Z","shell.execute_reply.started":"2022-11-28T21:39:20.966538Z","shell.execute_reply":"2022-11-28T21:39:29.881403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(score.shape)","metadata":{"execution":{"iopub.status.busy":"2022-11-28T21:39:36.662940Z","iopub.execute_input":"2022-11-28T21:39:36.663593Z","iopub.status.idle":"2022-11-28T21:39:36.669365Z","shell.execute_reply.started":"2022-11-28T21:39:36.663556Z","shell.execute_reply":"2022-11-28T21:39:36.668298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfx = pd.DataFrame(list(np.arange(maxlen)), columns =['Keys'] );\ndfy2=pd.DataFrame(score[0][0],columns=list(np.arange(maxlen)));\nplt.figure(figsize=(100.0,100.0));\nplt.title(\"Attention scores\");\nplt.xlabel('Keys',size=maxlen);\nplt.ylabel('Queries',size=maxlen);\nplt.plot();\nsns.heatmap(dfy2,fmt=\".3f\",annot=True,linewidths=2,square=True,cmap='twilight');","metadata":{"execution":{"iopub.status.busy":"2022-11-28T21:39:38.274080Z","iopub.execute_input":"2022-11-28T21:39:38.274809Z","iopub.status.idle":"2022-11-28T21:39:42.515863Z","shell.execute_reply.started":"2022-11-28T21:39:38.274770Z","shell.execute_reply":"2022-11-28T21:39:42.514970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfx = pd.DataFrame(list(np.arange(maxlen)), columns =['Keys'] );\ndfy2=pd.DataFrame(score[0][1],columns=list(np.arange(maxlen)));\nplt.figure(figsize=(100.0,100.0));\nplt.title(\"Attention scores\");\nplt.xlabel('Keys',size=maxlen);\nplt.ylabel('Queries',size=maxlen);\nplt.plot();\nsns.heatmap(dfy2,fmt=\".3f\",annot=True,linewidths=2,square=True,cmap='twilight');","metadata":{"execution":{"iopub.status.busy":"2022-11-28T21:39:42.517459Z","iopub.execute_input":"2022-11-28T21:39:42.517720Z","iopub.status.idle":"2022-11-28T21:39:46.498256Z","shell.execute_reply.started":"2022-11-28T21:39:42.517697Z","shell.execute_reply":"2022-11-28T21:39:46.497226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfx = pd.DataFrame(list(np.arange(maxlen)), columns =['Keys'] );\ndfy2=pd.DataFrame(score[0][2],columns=list(np.arange(maxlen)));\nplt.figure(figsize=(100.0,100.0));\nplt.title(\"Attention scores\");\nplt.xlabel('Keys',size=maxlen);\nplt.ylabel('Queries',size=maxlen);\nplt.plot();\nsns.heatmap(dfy2,fmt=\".3f\",annot=True,linewidths=2,square=True,cmap='twilight');","metadata":{"execution":{"iopub.status.busy":"2022-11-28T21:39:46.500194Z","iopub.execute_input":"2022-11-28T21:39:46.500652Z","iopub.status.idle":"2022-11-28T21:39:50.306707Z","shell.execute_reply.started":"2022-11-28T21:39:46.500612Z","shell.execute_reply":"2022-11-28T21:39:50.305722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfx = pd.DataFrame(list(np.arange(maxlen)), columns =['Keys'] );\ndfy2=pd.DataFrame(score[0][3],columns=list(np.arange(maxlen)));\nplt.figure(figsize=(100.0,100.0));\nplt.title(\"Attention scores\");\nplt.xlabel('Keys',size=maxlen);\nplt.ylabel('Queries',size=maxlen);\nplt.plot();\nsns.heatmap(dfy2,fmt=\".3f\",annot=True,linewidths=2,square=True,cmap='twilight');","metadata":{"execution":{"iopub.status.busy":"2022-11-28T21:39:50.308332Z","iopub.execute_input":"2022-11-28T21:39:50.308714Z","iopub.status.idle":"2022-11-28T21:39:54.028561Z","shell.execute_reply.started":"2022-11-28T21:39:50.308677Z","shell.execute_reply":"2022-11-28T21:39:54.027541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfx = pd.DataFrame(list(np.arange(maxlen)), columns =['Keys'] );\ndfy2=pd.DataFrame(score[0][4],columns=list(np.arange(maxlen)));\nplt.figure(figsize=(100.0,100.0));\nplt.title(\"Attention scores\");\nplt.xlabel('Keys',size=maxlen);\nplt.ylabel('Queries',size=maxlen);\nplt.plot();\nsns.heatmap(dfy2,fmt=\".3f\",annot=True,linewidths=2,square=True,cmap='twilight');","metadata":{"execution":{"iopub.status.busy":"2022-11-28T21:39:54.030964Z","iopub.execute_input":"2022-11-28T21:39:54.031738Z","iopub.status.idle":"2022-11-28T21:39:58.219529Z","shell.execute_reply.started":"2022-11-28T21:39:54.031699Z","shell.execute_reply":"2022-11-28T21:39:58.218495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfx = pd.DataFrame(list(np.arange(maxlen)), columns =['Keys'] );\ndfy2=pd.DataFrame(score[0][5],columns=list(np.arange(maxlen)));\nplt.figure(figsize=(100.0,100.0));\nplt.title(\"Attention scores\");\nplt.xlabel('Keys',size=maxlen);\nplt.ylabel('Queries',size=maxlen);\nplt.plot();\nsns.heatmap(dfy2,fmt=\".3f\",annot=True,linewidths=2,square=True,cmap='twilight');","metadata":{"execution":{"iopub.status.busy":"2022-11-28T21:39:58.221238Z","iopub.execute_input":"2022-11-28T21:39:58.221577Z","iopub.status.idle":"2022-11-28T21:40:01.944263Z","shell.execute_reply.started":"2022-11-28T21:39:58.221545Z","shell.execute_reply":"2022-11-28T21:40:01.943085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfx = pd.DataFrame(list(np.arange(maxlen)), columns =['Keys'] );\ndfy2=pd.DataFrame(score[0][6],columns=list(np.arange(maxlen)));\nplt.figure(figsize=(100.0,100.0));\nplt.title(\"Attention scores\");\nplt.xlabel('Keys',size=maxlen);\nplt.ylabel('Queries',size=maxlen);\nplt.plot();\nsns.heatmap(dfy2,fmt=\".3f\",annot=True,linewidths=2,square=True,cmap='twilight');","metadata":{"execution":{"iopub.status.busy":"2022-11-28T21:40:01.945943Z","iopub.execute_input":"2022-11-28T21:40:01.946565Z","iopub.status.idle":"2022-11-28T21:40:05.765383Z","shell.execute_reply.started":"2022-11-28T21:40:01.946519Z","shell.execute_reply":"2022-11-28T21:40:05.764376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfx = pd.DataFrame(list(np.arange(maxlen)), columns =['Keys'] );\ndfy2=pd.DataFrame(score[0][7],columns=list(np.arange(maxlen)));\nplt.figure(figsize=(100.0,100.0));\nplt.title(\"Attention scores\");\nplt.xlabel('Keys',size=maxlen);\nplt.ylabel('Queries',size=maxlen);\nplt.plot();\nsns.heatmap(dfy2,fmt=\".3f\",annot=True,linewidths=2,square=True,cmap='twilight');","metadata":{"execution":{"iopub.status.busy":"2022-11-28T21:40:05.769671Z","iopub.execute_input":"2022-11-28T21:40:05.771844Z","iopub.status.idle":"2022-11-28T21:40:09.981560Z","shell.execute_reply.started":"2022-11-28T21:40:05.771808Z","shell.execute_reply":"2022-11-28T21:40:09.980355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}