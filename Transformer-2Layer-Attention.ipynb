{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport random\n\n#replace with pytorch lightning\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n\nfrom sklearn.preprocessing import MinMaxScaler    \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport string","metadata":{"execution":{"iopub.status.busy":"2022-11-26T17:58:46.897835Z","iopub.execute_input":"2022-11-26T17:58:46.898538Z","iopub.status.idle":"2022-11-26T17:58:49.599363Z","shell.execute_reply.started":"2022-11-26T17:58:46.898445Z","shell.execute_reply":"2022-11-26T17:58:49.597991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice = torch.device(device)\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-11-26T17:59:07.785106Z","iopub.execute_input":"2022-11-26T17:59:07.785745Z","iopub.status.idle":"2022-11-26T17:59:07.854377Z","shell.execute_reply.started":"2022-11-26T17:59:07.785707Z","shell.execute_reply":"2022-11-26T17:59:07.853261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n    \"\"\"\n    Mask the upper half of the dot product matrix in self attention.\n    This prevents flow of information from future tokens to current token.\n    1's in the lower triangle, counting from the lower right corner.\n    \"\"\"\n    i=torch.range(1,n_dest)[:,None]\n    j=torch.range(1,n_src)\n    m = i >= j - n_src + n_dest\n    mask=m.bool()\n    return ~mask\n#     mask=torch.reshape(mask, [1, n_dest, n_src])\n#     mult=[batch_size,1,1]\n#     return torch.tile(mask,mult);","metadata":{"execution":{"iopub.status.busy":"2022-11-26T18:00:02.181326Z","iopub.execute_input":"2022-11-26T18:00:02.182004Z","iopub.status.idle":"2022-11-26T18:00:02.187903Z","shell.execute_reply.started":"2022-11-26T18:00:02.181969Z","shell.execute_reply":"2022-11-26T18:00:02.186957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"causal_attention_mask(2,5,5,torch.bool)","metadata":{"execution":{"iopub.status.busy":"2022-11-26T18:00:03.483949Z","iopub.execute_input":"2022-11-26T18:00:03.484345Z","iopub.status.idle":"2022-11-26T18:00:03.497994Z","shell.execute_reply.started":"2022-11-26T18:00:03.484291Z","shell.execute_reply":"2022-11-26T18:00:03.496888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def padding_mask(input):\n    # Create mask which marks the zero padding values in the input by a 1\n#     print(input)\n#     input=torch.tensor(input['train']['input_ids'])\n    mask=torch.eq(input, torch.zeros_like(input))\n\n \n    return mask","metadata":{"execution":{"iopub.status.busy":"2022-11-26T18:00:04.641132Z","iopub.execute_input":"2022-11-26T18:00:04.641518Z","iopub.status.idle":"2022-11-26T18:00:04.646744Z","shell.execute_reply.started":"2022-11-26T18:00:04.641487Z","shell.execute_reply":"2022-11-26T18:00:04.645734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"padding_mask(torch.tensor([[1,2,3,0,0,0],[2,0,0,0,0,0]]))","metadata":{"execution":{"iopub.status.busy":"2022-11-24T19:15:32.213074Z","iopub.execute_input":"2022-11-24T19:15:32.213429Z","iopub.status.idle":"2022-11-24T19:15:32.223241Z","shell.execute_reply.started":"2022-11-24T19:15:32.213398Z","shell.execute_reply":"2022-11-24T19:15:32.222075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(self, embed_dim, num_heads,batch_first,rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.attention = nn.MultiheadAttention(embed_dim,num_heads,batch_first=batch_first)\n\n    def forward(self, inputs,pad_mask):\n        input_shape = inputs.size()\n        batch_size = input_shape[0]\n        seq_len = input_shape[1]\n#         pad_mask=padding_mask(inputs)\n        pad_mask.to(device)\n        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, torch.bool).to(device)\n        attention_output,a = self.attention(inputs, inputs,inputs, key_padding_mask=pad_mask, attn_mask=causal_mask,need_weights=True,average_attn_weights=False)\n        return attention_output,a","metadata":{"execution":{"iopub.status.busy":"2022-11-26T18:00:09.087704Z","iopub.execute_input":"2022-11-26T18:00:09.088313Z","iopub.status.idle":"2022-11-26T18:00:09.095900Z","shell.execute_reply.started":"2022-11-26T18:00:09.088275Z","shell.execute_reply":"2022-11-26T18:00:09.094923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TokenAndPositionEmbedding(nn.Module):\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        super(TokenAndPositionEmbedding, self).__init__()\n        self.token_emb = nn.Embedding(vocab_size, embed_dim,max_norm=1)\n        self.pos_emb = nn.Embedding(maxlen,embed_dim)\n\n    def forward(self, x):\n        maxlen = x.size()[-1]\n        pad_mask=padding_mask(x)\n        positions = torch.range(start=0, end=maxlen-1, step=1,dtype=torch.int32).to(device)\n        positions = self.pos_emb(positions)\n        x = self.token_emb(x)\n        return (x + positions),pad_mask","metadata":{"execution":{"iopub.status.busy":"2022-11-26T18:00:12.040881Z","iopub.execute_input":"2022-11-26T18:00:12.041260Z","iopub.status.idle":"2022-11-26T18:00:12.050521Z","shell.execute_reply.started":"2022-11-26T18:00:12.041226Z","shell.execute_reply":"2022-11-26T18:00:12.049400Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size =50257 #28996  # Only consider the top 20k words\nmaxlen = 20  # Max sequence size\nembed_dim = 256  # Embedding size for each token\nnum_heads = 8  # Number of attention heads\nfeed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n        self.transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim,True)\n        self.outputs= nn.LazyLinear(vocab_size)\n        \n    def forward(self, x):\n        x,pad_mask = self.embedding_layer(x)\n#         print(x,\"maximum=\",torch.max(x))\n        x,a = self.transformer_block(x,pad_mask)\n        x,a = self.transformer_block(x,pad_mask)\n#         print(x)\n        x = self.outputs(x)\n        \n        return x,a\n","metadata":{"execution":{"iopub.status.busy":"2022-11-26T18:00:14.295062Z","iopub.execute_input":"2022-11-26T18:00:14.295460Z","iopub.status.idle":"2022-11-26T18:00:14.302500Z","shell.execute_reply.started":"2022-11-26T18:00:14.295428Z","shell.execute_reply":"2022-11-26T18:00:14.301500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport re\ndirectories = [\n    \"/kaggle/input/aclimdb-v1/aclImdb/train/pos\",\n    \"/kaggle/input/aclimdb-v1/aclImdb/train/neg\",\n    \"/kaggle/input/aclimdb-v1/aclImdb/test/pos\",\n    \"/kaggle/input/aclimdb-v1/aclImdb/test/neg\",\n]\n\nfrom datasets import load_dataset\nfilenames = []\nfor dir in directories:\n    for f in os.listdir(dir):\n        filenames.append(os.path.join(dir, f))\n\ndataset = load_dataset(\"text\", data_files=filenames)\n\ndef processing(s):\n  s['text']=s['text'].lower()\n  s['text']=re.sub(\"<br />\", \" \", s['text'])\n  s['text']=re.sub(f\"([{string.punctuation}])\", r\" \\1\", s['text'])\n  return s\n\ndataset=dataset.map(processing)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-11-24T17:49:16.259722Z","iopub.execute_input":"2022-11-24T17:49:16.260411Z","iopub.status.idle":"2022-11-24T18:02:03.996988Z","shell.execute_reply.started":"2022-11-24T17:49:16.260376Z","shell.execute_reply":"2022-11-24T18:02:03.995999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers import AutoTokenizer\n# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\nfrom transformers import GPT2Tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")","metadata":{"execution":{"iopub.status.busy":"2022-11-26T18:00:18.125322Z","iopub.execute_input":"2022-11-26T18:00:18.126365Z","iopub.status.idle":"2022-11-26T18:00:23.860788Z","shell.execute_reply.started":"2022-11-26T18:00:18.126298Z","shell.execute_reply":"2022-11-26T18:00:23.859689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = dataset.map(lambda dataset: tokenizer(dataset[\"text\"],truncation=True, max_length=maxlen))","metadata":{"execution":{"iopub.status.busy":"2022-11-24T18:07:07.911436Z","iopub.execute_input":"2022-11-24T18:07:07.911839Z","iopub.status.idle":"2022-11-24T18:09:05.733028Z","shell.execute_reply.started":"2022-11-24T18:07:07.911803Z","shell.execute_reply":"2022-11-24T18:09:05.732055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def padding(s):\n    if len(s['input_ids'])<maxlen :\n        s['input_ids']=s['input_ids']+[0]*(maxlen-len(s['input_ids']))\n    return s\n                                           \ndataset=dataset.map(padding)\n","metadata":{"execution":{"iopub.status.busy":"2022-11-24T18:13:55.808220Z","iopub.execute_input":"2022-11-24T18:13:55.808595Z","iopub.status.idle":"2022-11-24T18:14:04.576526Z","shell.execute_reply.started":"2022-11-24T18:13:55.808565Z","shell.execute_reply":"2022-11-24T18:14:04.575484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2022-11-23T19:37:34.968095Z","iopub.execute_input":"2022-11-23T19:37:34.968708Z","iopub.status.idle":"2022-11-23T19:37:34.976015Z","shell.execute_reply.started":"2022-11-23T19:37:34.968669Z","shell.execute_reply":"2022-11-23T19:37:34.974984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=Model()\n# model.load_state_dict(torch.load(\"/kaggle/input/weights/transformer_weights-2.pth\")) \nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-11-26T18:00:53.740133Z","iopub.execute_input":"2022-11-26T18:00:53.740521Z","iopub.status.idle":"2022-11-26T18:00:57.908536Z","shell.execute_reply.started":"2022-11-26T18:00:53.740489Z","shell.execute_reply":"2022-11-26T18:00:57.907606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<!-- # model= Model()\n# model.to(device) -->","metadata":{"execution":{"iopub.status.busy":"2022-11-23T19:37:34.977324Z","iopub.execute_input":"2022-11-23T19:37:34.977741Z","iopub.status.idle":"2022-11-23T19:37:35.372676Z","shell.execute_reply.started":"2022-11-23T19:37:34.977705Z","shell.execute_reply":"2022-11-23T19:37:35.371685Z"}}},{"cell_type":"code","source":"class TextGenerator(nn.Module):\n    \"\"\"A callback to generate text from a trained model.\n    1. Feed some starting prompt to the model\n    2. Predict probabilities for the next token\n    3. Sample the next token and add it to the next input\n\n    Arguments:\n        max_tokens: Integer, the number of tokens to be generated after prompt.\n        start_tokens: List of integers, the token indices for the starting prompt.\n        index_to_word: List of strings, obtained from the TextVectorization layer.\n        top_k: Integer, sample from the `top_k` token predictions.\n        print_every: Integer, print after this many epochs.\n    \"\"\"\n\n    def __init__(\n        self, max_tokens, start_tokens, top_k=10, print_every=1\n    ):\n        self.max_tokens = max_tokens\n        self.start_tokens = start_tokens\n#         self.index_to_word = index_to_word\n        self.print_every = print_every\n        self.k = top_k\n\n    def sample_from(self, logits):\n        logits, indices = torch.topk(logits, k=self.k, sorted=True)\n        logits=logits.cpu()\n        indices=indices.cpu()\n        indices = np.asarray(indices).astype(\"int32\")\n       \n        softmax=nn.Softmax(dim=0)\n        preds = softmax(logits)\n        preds = np.asarray(preds).astype(\"float32\")\n#         return np.random.choice(indices, p=preds) THIS IS THE CORRECT CODE, BUT HAD TO COMMENT IT AS\n#.        PROBABILITIES HAVE NAN AND I HAD TO VERIFY PIPELINE, BELOW LINE WILL BE REMOVED ONCE NAN ISSUE \n#.        IS RESOLVED\n        return np.random.choice(indices, p=preds)\n#         return np.random.choice(5, 1, p=[0.1, 0, 0.3, 0.6, 0])\n\n    def detokenize(self, number):\n        return tokenizer.decode(number)\n\n    def on_epoch_end(self, epoch, logs=None):\n        start_tokens = [_ for _ in self.start_tokens]\n        if (epoch + 1) % self.print_every != 0:\n            return\n        num_tokens_generated = 0\n        tokens_generated = []\n        attention_scores=[]\n        while num_tokens_generated <= self.max_tokens:\n            pad_len = maxlen - len(start_tokens)\n            sample_index = len(start_tokens) - 1\n            if pad_len < 0:\n                data = start_tokens[:maxlen]\n                sample_index = maxlen - 1\n            elif pad_len > 0:\n                data = start_tokens + [0] * pad_len\n            else:\n                data = start_tokens\n                \n            data = torch.Tensor(np.array([data])).type(torch.int32).to(device)\n            \n            y,attention_scores = model(data)\n            sample_token = self.sample_from(y[0][sample_index])\n            tokens_generated.append(sample_token)\n            start_tokens.append(sample_token)\n            num_tokens_generated = len(tokens_generated)\n        txt = \" \".join(\n            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n        )\n        print(f\"generated text:\\n{txt}\\n\")\n        return attention_scores\n\n# Tokenize starting prompt\n# word_to_index = {}\n# for index, word in enumerate(vocab):\n#     word_to_index[word] = index\n\nstart_prompt = \"the movie is good but\"\nstart_tokens=tokenizer(start_prompt)['input_ids']\n# start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\nnum_tokens_generated = 40\n# text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)","metadata":{"execution":{"iopub.status.busy":"2022-11-26T18:16:26.208570Z","iopub.execute_input":"2022-11-26T18:16:26.208944Z","iopub.status.idle":"2022-11-26T18:16:26.223481Z","shell.execute_reply.started":"2022-11-26T18:16:26.208913Z","shell.execute_reply":"2022-11-26T18:16:26.222226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\ntrain_dataset= Dataset.from_dict({\"id\": dataset['train']['input_ids']})\ntrain_dataset = train_dataset.with_format(\"torch\")","metadata":{"execution":{"iopub.status.busy":"2022-11-23T19:37:35.390463Z","iopub.execute_input":"2022-11-23T19:37:35.391040Z","iopub.status.idle":"2022-11-23T19:37:37.379199Z","shell.execute_reply.started":"2022-11-23T19:37:35.391003Z","shell.execute_reply":"2022-11-23T19:37:37.377983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count=0\nTEST=[]\ntrain_loader=DataLoader(train_dataset,batch_size=50,shuffle=True)\nfor i in train_loader:\n    TEST.append(random.choice(i['id']))\n    if count>100:\n        break\n    count+=1","metadata":{"execution":{"iopub.status.busy":"2022-11-23T19:37:37.383762Z","iopub.execute_input":"2022-11-23T19:37:37.384709Z","iopub.status.idle":"2022-11-23T19:37:38.360814Z","shell.execute_reply.started":"2022-11-23T19:37:37.384670Z","shell.execute_reply":"2022-11-23T19:37:38.359756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TEST=torch.stack(TEST,0) \n# print(TEST[:,:])","metadata":{"execution":{"iopub.status.busy":"2022-11-23T19:37:38.362369Z","iopub.execute_input":"2022-11-23T19:37:38.362735Z","iopub.status.idle":"2022-11-23T19:37:38.367810Z","shell.execute_reply.started":"2022-11-23T19:37:38.362697Z","shell.execute_reply":"2022-11-23T19:37:38.366821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader=DataLoader(train_dataset,batch_size=50,shuffle=True)\noptim=torch.optim.AdamW(model.parameters(),lr=1e-4)\nloss_fn=torch.nn.CrossEntropyLoss()\ncount=0\nloss_stats = {\n    'test': [],\n}\nfor epoch in tqdm(range(20)):\n    for batch in tqdm(train_loader):\n        optim.zero_grad()\n#         print(batch['id'][:,:-1])\n        input_ids=batch['id'][:,:-1].to(device)\n#         print(input_ids.shape())\n        labels=batch['id'][:,1:].to(device)\n        outputs,attention_scores=model.forward(input_ids)\n        labels=nn.functional.one_hot(labels,num_classes=vocab_size).type(torch.float)\n        loss=loss_fn(outputs,labels)\n        loss.backward()\n        optim.step()\n    \n    with torch.no_grad():\n        TextGenerator(40, start_tokens).on_epoch_end(epoch);\n        test_input=TEST[:,:-1].to(device)\n        test_output=TEST[:,1:].to(device)\n        outputs,attention_scores=model.forward(test_input)\n        labels=nn.functional.one_hot(test_output,num_classes=vocab_size).type(torch.float)\n        loss=loss_fn(outputs,labels).cpu().item()\n        loss_stats['test'].append(loss)\n        ","metadata":{"execution":{"iopub.status.busy":"2022-11-23T20:02:26.594334Z","iopub.execute_input":"2022-11-23T20:02:26.594727Z","iopub.status.idle":"2022-11-23T20:05:58.721767Z","shell.execute_reply.started":"2022-11-23T20:02:26.594694Z","shell.execute_reply":"2022-11-23T20:05:58.720631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loss_df = pd.DataFrame.from_dict(loss_stats).reset_index().melt(id_vars=['index']).rename(columns={\"index\":\"epochs\"})\n# Plot the dataframes\nfig,axes = plt.subplots(nrows=1, ncols=1, figsize=(20,7))\nsns.lineplot(data=test_loss_df, x = \"epochs\", y=\"value\", hue=\"variable\",  ax=axes).set_title('TestLoss')\n","metadata":{"execution":{"iopub.status.busy":"2022-11-23T20:06:09.792378Z","iopub.execute_input":"2022-11-23T20:06:09.793293Z","iopub.status.idle":"2022-11-23T20:06:10.077278Z","shell.execute_reply.started":"2022-11-23T20:06:09.793257Z","shell.execute_reply":"2022-11-23T20:06:10.076327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir /kaggle/working/saved_models/\ntorch.save(model.state_dict(),\"/kaggle/working/saved_models/transformer_weights.pth\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    score=TextGenerator(10, start_tokens).on_epoch_end(1);\nscore=score.cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2022-11-26T18:12:33.160945Z","iopub.execute_input":"2022-11-26T18:12:33.161345Z","iopub.status.idle":"2022-11-26T18:12:33.187773Z","shell.execute_reply.started":"2022-11-26T18:12:33.161297Z","shell.execute_reply":"2022-11-26T18:12:33.186786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(score.shape)","metadata":{"execution":{"iopub.status.busy":"2022-11-26T18:03:23.769180Z","iopub.execute_input":"2022-11-26T18:03:23.769866Z","iopub.status.idle":"2022-11-26T18:03:23.778438Z","shell.execute_reply.started":"2022-11-26T18:03:23.769829Z","shell.execute_reply":"2022-11-26T18:03:23.777457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfx = pd.DataFrame(list(np.arange(maxlen)), columns =['Keys'] );\ndfy2=pd.DataFrame(score[0][0],columns=list(np.arange(maxlen)));\nplt.figure(figsize=(100.0,100.0));\nplt.title(\"Attention scores\");\nplt.xlabel('Keys',size=maxlen);\nplt.ylabel('Queries',size=maxlen);\nplt.plot();\nsns.heatmap(dfy2,fmt=\".3f\",annot=True,linewidths=2,square=True,cmap='twilight');","metadata":{"execution":{"iopub.status.busy":"2022-11-26T18:03:40.192502Z","iopub.execute_input":"2022-11-26T18:03:40.193512Z","iopub.status.idle":"2022-11-26T18:03:44.280544Z","shell.execute_reply.started":"2022-11-26T18:03:40.193458Z","shell.execute_reply":"2022-11-26T18:03:44.279343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfx = pd.DataFrame(list(np.arange(maxlen)), columns =['Keys'] );\ndfy2=pd.DataFrame(score[0][1],columns=list(np.arange(maxlen)));\nplt.figure(figsize=(100.0,100.0));\nplt.title(\"Attention scores\");\nplt.xlabel('Keys',size=maxlen);\nplt.ylabel('Queries',size=maxlen);\nplt.plot();\nsns.heatmap(dfy2,fmt=\".3f\",annot=True,linewidths=2,square=True,cmap='twilight');","metadata":{"execution":{"iopub.status.busy":"2022-11-26T18:04:46.929245Z","iopub.execute_input":"2022-11-26T18:04:46.929884Z","iopub.status.idle":"2022-11-26T18:04:51.274930Z","shell.execute_reply.started":"2022-11-26T18:04:46.929848Z","shell.execute_reply":"2022-11-26T18:04:51.273898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfx = pd.DataFrame(list(np.arange(maxlen)), columns =['Keys'] );\ndfy2=pd.DataFrame(score[0][2],columns=list(np.arange(maxlen)));\nplt.figure(figsize=(100.0,100.0));\nplt.title(\"Attention scores\");\nplt.xlabel('Keys',size=maxlen);\nplt.ylabel('Queries',size=maxlen);\nplt.plot();\nsns.heatmap(dfy2,fmt=\".3f\",annot=True,linewidths=2,square=True,cmap='twilight');","metadata":{"execution":{"iopub.status.busy":"2022-11-26T18:04:58.271889Z","iopub.execute_input":"2022-11-26T18:04:58.272288Z","iopub.status.idle":"2022-11-26T18:05:02.055173Z","shell.execute_reply.started":"2022-11-26T18:04:58.272247Z","shell.execute_reply":"2022-11-26T18:05:02.054237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfx = pd.DataFrame(list(np.arange(maxlen)), columns =['Keys'] );\ndfy2=pd.DataFrame(score[0][3],columns=list(np.arange(maxlen)));\nplt.figure(figsize=(100.0,100.0));\nplt.title(\"Attention scores\");\nplt.xlabel('Keys',size=maxlen);\nplt.ylabel('Queries',size=maxlen);\nplt.plot();\nsns.heatmap(dfy2,fmt=\".3f\",annot=True,linewidths=2,square=True,cmap='twilight');","metadata":{"execution":{"iopub.status.busy":"2022-11-26T18:05:10.989096Z","iopub.execute_input":"2022-11-26T18:05:10.989491Z","iopub.status.idle":"2022-11-26T18:05:14.643606Z","shell.execute_reply.started":"2022-11-26T18:05:10.989457Z","shell.execute_reply":"2022-11-26T18:05:14.642398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfx = pd.DataFrame(list(np.arange(maxlen)), columns =['Keys'] );\ndfy2=pd.DataFrame(score[0][4],columns=list(np.arange(maxlen)));\nplt.figure(figsize=(100.0,100.0));\nplt.title(\"Attention scores\");\nplt.xlabel('Keys',size=maxlen);\nplt.ylabel('Queries',size=maxlen);\nplt.plot();\nsns.heatmap(dfy2,fmt=\".3f\",annot=True,linewidths=2,square=True,cmap='twilight');","metadata":{"execution":{"iopub.status.busy":"2022-11-26T18:05:21.678963Z","iopub.execute_input":"2022-11-26T18:05:21.679343Z","iopub.status.idle":"2022-11-26T18:05:25.360218Z","shell.execute_reply.started":"2022-11-26T18:05:21.679294Z","shell.execute_reply":"2022-11-26T18:05:25.359105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfx = pd.DataFrame(list(np.arange(maxlen)), columns =['Keys'] );\ndfy2=pd.DataFrame(score[0][5],columns=list(np.arange(maxlen)));\nplt.figure(figsize=(100.0,100.0));\nplt.title(\"Attention scores\");\nplt.xlabel('Keys',size=maxlen);\nplt.ylabel('Queries',size=maxlen);\nplt.plot();\nsns.heatmap(dfy2,fmt=\".3f\",annot=True,linewidths=2,square=True,cmap='twilight');","metadata":{"execution":{"iopub.status.busy":"2022-11-26T18:05:30.314790Z","iopub.execute_input":"2022-11-26T18:05:30.315159Z","iopub.status.idle":"2022-11-26T18:05:34.177123Z","shell.execute_reply.started":"2022-11-26T18:05:30.315128Z","shell.execute_reply":"2022-11-26T18:05:34.176025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfx = pd.DataFrame(list(np.arange(maxlen)), columns =['Keys'] );\ndfy2=pd.DataFrame(score[0][6],columns=list(np.arange(maxlen)));\nplt.figure(figsize=(100.0,100.0));\nplt.title(\"Attention scores\");\nplt.xlabel('Keys',size=maxlen);\nplt.ylabel('Queries',size=maxlen);\nplt.plot();\nsns.heatmap(dfy2,fmt=\".3f\",annot=True,linewidths=2,square=True,cmap='twilight');","metadata":{"execution":{"iopub.status.busy":"2022-11-26T18:05:54.993957Z","iopub.execute_input":"2022-11-26T18:05:54.994360Z","iopub.status.idle":"2022-11-26T18:05:59.048257Z","shell.execute_reply.started":"2022-11-26T18:05:54.994308Z","shell.execute_reply":"2022-11-26T18:05:59.047208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfx = pd.DataFrame(list(np.arange(maxlen)), columns =['Keys'] );\ndfy2=pd.DataFrame(score[0][7],columns=list(np.arange(maxlen)));\nplt.figure(figsize=(100.0,100.0));\nplt.title(\"Attention scores\");\nplt.xlabel('Keys',size=maxlen);\nplt.ylabel('Queries',size=maxlen);\nplt.plot();\nsns.heatmap(dfy2,fmt=\".3f\",annot=True,linewidths=2,square=True,cmap='twilight');","metadata":{"execution":{"iopub.status.busy":"2022-11-26T18:06:03.461790Z","iopub.execute_input":"2022-11-26T18:06:03.462234Z","iopub.status.idle":"2022-11-26T18:06:07.268404Z","shell.execute_reply.started":"2022-11-26T18:06:03.462197Z","shell.execute_reply":"2022-11-26T18:06:07.267291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}