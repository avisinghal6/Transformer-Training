{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport random\n\n#replace with pytorch lightning\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n\nfrom sklearn.preprocessing import MinMaxScaler    \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport string","metadata":{"execution":{"iopub.status.busy":"2022-11-30T23:53:24.928730Z","iopub.execute_input":"2022-11-30T23:53:24.929974Z","iopub.status.idle":"2022-11-30T23:53:24.937238Z","shell.execute_reply.started":"2022-11-30T23:53:24.929926Z","shell.execute_reply":"2022-11-30T23:53:24.936241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"wikitext\",\"wikitext-103-raw-v1\", split=\"train\")","metadata":{"execution":{"iopub.status.busy":"2022-11-30T23:53:26.013601Z","iopub.execute_input":"2022-11-30T23:53:26.013989Z","iopub.status.idle":"2022-11-30T23:53:26.426349Z","shell.execute_reply.started":"2022-11-30T23:53:26.013955Z","shell.execute_reply":"2022-11-30T23:53:26.425404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset=dataset.select(list(np.arange(0,600000)))\nprint(dataset)","metadata":{"execution":{"iopub.status.busy":"2022-11-30T23:53:27.458040Z","iopub.execute_input":"2022-11-30T23:53:27.458706Z","iopub.status.idle":"2022-11-30T23:53:40.106242Z","shell.execute_reply.started":"2022-11-30T23:53:27.458668Z","shell.execute_reply":"2022-11-30T23:53:40.105118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice = torch.device(device)\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-11-30T23:53:40.109266Z","iopub.execute_input":"2022-11-30T23:53:40.109590Z","iopub.status.idle":"2022-11-30T23:53:40.118014Z","shell.execute_reply.started":"2022-11-30T23:53:40.109560Z","shell.execute_reply":"2022-11-30T23:53:40.116991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n    \"\"\"\n    Mask the upper half of the dot product matrix in self attention.\n    This prevents flow of information from future tokens to current token.\n    1's in the lower triangle, counting from the lower right corner.\n    \"\"\"\n    i=torch.range(1,n_dest)[:,None]\n    j=torch.range(1,n_src)\n    m = i >= j - n_src + n_dest\n    mask=m.bool()\n    return ~mask\n#     mask=torch.reshape(mask, [1, n_dest, n_src])\n#     mult=[batch_size,1,1]\n#     return torch.tile(mask,mult);","metadata":{"execution":{"iopub.status.busy":"2022-11-30T23:53:47.469728Z","iopub.execute_input":"2022-11-30T23:53:47.470119Z","iopub.status.idle":"2022-11-30T23:53:47.476366Z","shell.execute_reply.started":"2022-11-30T23:53:47.470086Z","shell.execute_reply":"2022-11-30T23:53:47.475305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"causal_attention_mask(2,5,5,torch.bool)","metadata":{"execution":{"iopub.status.busy":"2022-12-01T00:39:37.649696Z","iopub.execute_input":"2022-12-01T00:39:37.650754Z","iopub.status.idle":"2022-12-01T00:39:37.660081Z","shell.execute_reply.started":"2022-12-01T00:39:37.650710Z","shell.execute_reply":"2022-12-01T00:39:37.658873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def padding_mask(input):\n    # Create mask which marks the zero padding values in the input by a 1\n#     print(input)\n#     input=torch.tensor(input['train']['input_ids'])\n    mask=torch.eq(input, torch.zeros_like(input))\n\n \n    return mask","metadata":{"execution":{"iopub.status.busy":"2022-11-30T23:53:55.259750Z","iopub.execute_input":"2022-11-30T23:53:55.260447Z","iopub.status.idle":"2022-11-30T23:53:55.265336Z","shell.execute_reply.started":"2022-11-30T23:53:55.260400Z","shell.execute_reply":"2022-11-30T23:53:55.264358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"padding_mask(torch.tensor([[1,2,3,0,0,0],[2,0,0,0,0,0]]))","metadata":{"execution":{"iopub.status.busy":"2022-12-01T00:39:43.792544Z","iopub.execute_input":"2022-12-01T00:39:43.792956Z","iopub.status.idle":"2022-12-01T00:39:43.803536Z","shell.execute_reply.started":"2022-12-01T00:39:43.792922Z","shell.execute_reply":"2022-12-01T00:39:43.802429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(self, embed_dim, num_heads,batch_first,rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.attention = nn.MultiheadAttention(embed_dim,num_heads,batch_first=batch_first)\n\n    def forward(self, inputs,pad_mask):\n        input_shape = inputs.size()\n        batch_size = input_shape[0]\n        seq_len = input_shape[1]\n#         pad_mask=padding_mask(inputs)\n        pad_mask.to(device)\n        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, torch.bool).to(device)\n        attention_output,a = self.attention(inputs, inputs,inputs, key_padding_mask=pad_mask, attn_mask=causal_mask,need_weights=True,average_attn_weights=False)\n        return attention_output,a","metadata":{"execution":{"iopub.status.busy":"2022-11-30T23:53:59.115679Z","iopub.execute_input":"2022-11-30T23:53:59.116641Z","iopub.status.idle":"2022-11-30T23:53:59.123999Z","shell.execute_reply.started":"2022-11-30T23:53:59.116601Z","shell.execute_reply":"2022-11-30T23:53:59.122896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TokenAndPositionEmbedding(nn.Module):\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        super(TokenAndPositionEmbedding, self).__init__()\n        self.token_emb = nn.Embedding(vocab_size, embed_dim,max_norm=1)\n        self.pos_emb = nn.Embedding(maxlen,embed_dim)\n\n    def forward(self, x):\n        maxlen = x.size()[-1]\n        pad_mask=padding_mask(x)\n        positions = torch.range(start=0, end=maxlen-1, step=1,dtype=torch.int32).to(device)\n        positions = self.pos_emb(positions)\n        x = self.token_emb(x)\n        return (x + positions),pad_mask","metadata":{"execution":{"iopub.status.busy":"2022-11-30T23:54:00.297778Z","iopub.execute_input":"2022-11-30T23:54:00.298553Z","iopub.status.idle":"2022-11-30T23:54:00.308948Z","shell.execute_reply.started":"2022-11-30T23:54:00.298515Z","shell.execute_reply":"2022-11-30T23:54:00.307906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size =50257 #28996  # Only consider the top 20k words\nmaxlen = 50  # Max sequence size\nembed_dim = 256  # Embedding size for each token\nnum_heads = 8  # Number of attention heads\nfeed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n        self.transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim,True)\n        self.outputs= nn.LazyLinear(vocab_size)\n        \n    def forward(self, x):\n        x,pad_mask = self.embedding_layer(x)\n#         print(x,\"maximum=\",torch.max(x))\n        x,a = self.transformer_block(x,pad_mask)\n#         print(x)\n        x = self.outputs(x)\n        \n        return x,a\n","metadata":{"execution":{"iopub.status.busy":"2022-11-30T23:54:04.801591Z","iopub.execute_input":"2022-11-30T23:54:04.801977Z","iopub.status.idle":"2022-11-30T23:54:04.809960Z","shell.execute_reply.started":"2022-11-30T23:54:04.801942Z","shell.execute_reply":"2022-11-30T23:54:04.808711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport re\n# directories = [\n#     \"/kaggle/input/aclimdb-v1/aclImdb/train/pos\",\n#     \"/kaggle/input/aclimdb-v1/aclImdb/train/neg\",\n#     \"/kaggle/input/aclimdb-v1/aclImdb/test/pos\",\n#     \"/kaggle/input/aclimdb-v1/aclImdb/test/neg\",\n# ]\n\n# from datasets import load_dataset\n# filenames = []\n# for dir in directories:\n#     for f in os.listdir(dir):\n#         filenames.append(os.path.join(dir, f))\n\n# dataset = load_dataset(\"text\", data_files=filenames)\n\ndef processing(s):\n  s['text']=s['text'].lower()\n  s['text']=re.sub(\"<br />\", \" \", s['text'])\n  s['text']=re.sub(f\"([{string.punctuation}])\", r\" \\1\", s['text'])\n  return s\n\ndataset=dataset.map(processing)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-11-30T23:54:19.205870Z","iopub.execute_input":"2022-11-30T23:54:19.206392Z","iopub.status.idle":"2022-11-30T23:55:49.612031Z","shell.execute_reply.started":"2022-11-30T23:54:19.206356Z","shell.execute_reply":"2022-11-30T23:55:49.611099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = dataset.filter(lambda example: example['text']!=\"\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers import AutoTokenizer\n# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\nfrom transformers import GPT2Tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")","metadata":{"execution":{"iopub.status.busy":"2022-11-30T23:58:27.131258Z","iopub.execute_input":"2022-11-30T23:58:27.136078Z","iopub.status.idle":"2022-11-30T23:58:27.826627Z","shell.execute_reply.started":"2022-11-30T23:58:27.136020Z","shell.execute_reply":"2022-11-30T23:58:27.825636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = dataset.map(lambda dataset: tokenizer(dataset[\"text\"],truncation=True, max_length=maxlen))","metadata":{"execution":{"iopub.status.busy":"2022-11-30T23:58:31.642373Z","iopub.execute_input":"2022-11-30T23:58:31.642762Z","iopub.status.idle":"2022-12-01T00:05:47.054060Z","shell.execute_reply.started":"2022-11-30T23:58:31.642728Z","shell.execute_reply":"2022-12-01T00:05:47.053017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def padding(s):\n    if len(s['input_ids'])<maxlen :\n        s['input_ids']=s['input_ids']+[0]*(maxlen-len(s['input_ids']))\n    return s\n                                           \ndataset=dataset.map(padding)\n","metadata":{"execution":{"iopub.status.busy":"2022-12-01T00:13:22.108965Z","iopub.execute_input":"2022-12-01T00:13:22.109559Z","iopub.status.idle":"2022-12-01T00:14:28.650608Z","shell.execute_reply.started":"2022-12-01T00:13:22.109521Z","shell.execute_reply":"2022-12-01T00:14:28.649721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# count=0\n# for i in dataset:\n#     if len(i['input_ids'])!=50:\n#         print(len(i['input_ids']))","metadata":{"execution":{"iopub.status.busy":"2022-12-01T00:18:08.129032Z","iopub.execute_input":"2022-12-01T00:18:08.129410Z","iopub.status.idle":"2022-12-01T00:19:03.399719Z","shell.execute_reply.started":"2022-12-01T00:18:08.129372Z","shell.execute_reply":"2022-12-01T00:19:03.398723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=Model()\n# model.load_state_dict(torch.load(\"/kaggle/input/20len-1layer-weights/transformer_weights-6.pth\")) \nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-12-01T00:48:39.915263Z","iopub.execute_input":"2022-12-01T00:48:39.915995Z","iopub.status.idle":"2022-12-01T00:48:40.019961Z","shell.execute_reply.started":"2022-12-01T00:48:39.915936Z","shell.execute_reply":"2022-12-01T00:48:40.018775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<!-- # model= Model()\n# model.to(device) -->","metadata":{"execution":{"iopub.status.busy":"2022-11-23T19:37:34.977324Z","iopub.execute_input":"2022-11-23T19:37:34.977741Z","iopub.status.idle":"2022-11-23T19:37:35.372676Z","shell.execute_reply.started":"2022-11-23T19:37:34.977705Z","shell.execute_reply":"2022-11-23T19:37:35.371685Z"}}},{"cell_type":"code","source":"class TextGenerator(nn.Module):\n    \"\"\"A callback to generate text from a trained model.\n    1. Feed some starting prompt to the model\n    2. Predict probabilities for the next token\n    3. Sample the next token and add it to the next input\n\n    Arguments:\n        max_tokens: Integer, the number of tokens to be generated after prompt.\n        start_tokens: List of integers, the token indices for the starting prompt.\n        index_to_word: List of strings, obtained from the TextVectorization layer.\n        top_k: Integer, sample from the `top_k` token predictions.\n        print_every: Integer, print after this many epochs.\n    \"\"\"\n\n    def __init__(\n        self, max_tokens, start_tokens, top_k=10, print_every=1\n    ):\n        self.max_tokens = max_tokens\n        self.start_tokens = start_tokens\n#         self.index_to_word = index_to_word\n        self.print_every = print_every\n        self.k = top_k\n\n    def sample_from(self, logits):\n        logits, indices = torch.topk(logits, k=self.k, sorted=True)\n        logits=logits.cpu()\n        indices=indices.cpu()\n        indices = np.asarray(indices).astype(\"int32\")\n       \n        softmax=nn.Softmax(dim=0)\n        preds = softmax(logits)\n        preds = np.asarray(preds).astype(\"float32\")\n#         return np.random.choice(indices, p=preds) THIS IS THE CORRECT CODE, BUT HAD TO COMMENT IT AS\n#.        PROBABILITIES HAVE NAN AND I HAD TO VERIFY PIPELINE, BELOW LINE WILL BE REMOVED ONCE NAN ISSUE \n#.        IS RESOLVED\n        return np.random.choice(indices, p=preds)\n#         return np.random.choice(5, 1, p=[0.1, 0, 0.3, 0.6, 0])\n\n    def detokenize(self, number):\n        return tokenizer.decode(number)\n\n    def on_epoch_end(self, epoch, logs=None):\n        start_tokens = [_ for _ in self.start_tokens]\n        if (epoch + 1) % self.print_every != 0:\n            return\n        num_tokens_generated = 0\n        tokens_generated = []\n        attention_scores=[]\n        while num_tokens_generated <= self.max_tokens:\n            pad_len = maxlen - len(start_tokens)\n            sample_index = len(start_tokens) - 1\n            if pad_len < 0:\n                data = start_tokens[:maxlen]\n                sample_index = maxlen - 1\n            elif pad_len > 0:\n                data = start_tokens + [0] * pad_len\n            else:\n                data = start_tokens\n                \n            data = torch.Tensor(np.array([data])).type(torch.int32).to(device)\n            \n            y,attention_scores = model(data)\n            sample_token = self.sample_from(y[0][sample_index])\n            tokens_generated.append(sample_token)\n            start_tokens.append(sample_token)\n            num_tokens_generated = len(tokens_generated)\n        txt = \" \".join(\n            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n        )\n        print(f\"generated text:\\n{txt}\\n\")\n        return attention_scores[0]\n\n# Tokenize starting prompt\n# word_to_index = {}\n# for index, word in enumerate(vocab):\n#     word_to_index[word] = index\n\nstart_prompt = \"movie is better, movie is better\"\nstart_tokens=tokenizer(start_prompt)['input_ids']\n# start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\nnum_tokens_generated = 40\n# text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)","metadata":{"execution":{"iopub.status.busy":"2022-12-01T00:48:42.354485Z","iopub.execute_input":"2022-12-01T00:48:42.354871Z","iopub.status.idle":"2022-12-01T00:48:42.371436Z","shell.execute_reply.started":"2022-12-01T00:48:42.354817Z","shell.execute_reply":"2022-12-01T00:48:42.370432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\ntrain_dataset= Dataset.from_dict({\"id\": dataset['input_ids']})\ntrain_dataset = train_dataset.with_format(\"torch\")","metadata":{"execution":{"iopub.status.busy":"2022-12-01T00:21:29.834961Z","iopub.execute_input":"2022-12-01T00:21:29.835324Z","iopub.status.idle":"2022-12-01T00:21:46.934274Z","shell.execute_reply.started":"2022-12-01T00:21:29.835292Z","shell.execute_reply":"2022-12-01T00:21:46.933283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(train_dataset['id'][3])","metadata":{"execution":{"iopub.status.busy":"2022-12-01T00:43:23.982942Z","iopub.execute_input":"2022-12-01T00:43:23.983338Z","iopub.status.idle":"2022-12-01T00:43:27.214308Z","shell.execute_reply.started":"2022-12-01T00:43:23.983303Z","shell.execute_reply":"2022-12-01T00:43:27.213026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count=0\nTEST=[]\ntrain_loader=DataLoader(train_dataset,batch_size=50,shuffle=True)\nfor i in train_loader:\n    TEST.append(random.choice(i['id']))\n    if count>100:\n        break\n    count+=1","metadata":{"execution":{"iopub.status.busy":"2022-12-01T00:21:53.310217Z","iopub.execute_input":"2022-12-01T00:21:53.310589Z","iopub.status.idle":"2022-12-01T00:21:54.288675Z","shell.execute_reply.started":"2022-12-01T00:21:53.310557Z","shell.execute_reply":"2022-12-01T00:21:54.287623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TEST=torch.stack(TEST,0) \n# print(TEST[:,:])","metadata":{"execution":{"iopub.status.busy":"2022-12-01T00:21:56.527664Z","iopub.execute_input":"2022-12-01T00:21:56.528584Z","iopub.status.idle":"2022-12-01T00:21:56.534109Z","shell.execute_reply.started":"2022-12-01T00:21:56.528532Z","shell.execute_reply":"2022-12-01T00:21:56.532904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader=DataLoader(train_dataset,batch_size=50,shuffle=True)\noptim=torch.optim.AdamW(model.parameters(),lr=1e-4)\nloss_fn=torch.nn.CrossEntropyLoss()\ncount=0\nloss_stats = {\n    'test': [],\n}\nfor epoch in tqdm(range(20)):\n    for batch in tqdm(train_loader):\n        optim.zero_grad()\n#         print(batch['id'][:,:-1])\n        input_ids=batch['id'][:,:-1].to(device)\n#         print(input_ids.shape())\n        labels=batch['id'][:,1:].to(device)\n        outputs,attention_scores=model.forward(input_ids)\n        labels=nn.functional.one_hot(labels,num_classes=vocab_size).type(torch.float)\n        loss=loss_fn(outputs,labels)\n        loss.backward()\n        optim.step()\n    \n    with torch.no_grad():\n        TextGenerator(40, start_tokens).on_epoch_end(epoch);\n        test_input=TEST[:,:-1].to(device)\n        test_output=TEST[:,1:].to(device)\n        outputs,attention_scores=model.forward(test_input)\n        labels=nn.functional.one_hot(test_output,num_classes=vocab_size).type(torch.float)\n        loss=loss_fn(outputs,labels).cpu().item()\n        loss_stats['test'].append(loss)\n        ","metadata":{"execution":{"iopub.status.busy":"2022-12-01T00:21:58.632701Z","iopub.execute_input":"2022-12-01T00:21:58.634081Z","iopub.status.idle":"2022-12-01T00:34:20.930929Z","shell.execute_reply.started":"2022-12-01T00:21:58.634029Z","shell.execute_reply":"2022-12-01T00:34:20.929419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loss_df = pd.DataFrame.from_dict(loss_stats).reset_index().melt(id_vars=['index']).rename(columns={\"index\":\"epochs\"})\n# Plot the dataframes\nfig,axes = plt.subplots(nrows=1, ncols=1, figsize=(20,7))\nsns.lineplot(data=test_loss_df, x = \"epochs\", y=\"value\", hue=\"variable\",  ax=axes).set_title('TestLoss')\n","metadata":{"execution":{"iopub.status.busy":"2022-11-23T20:06:09.792378Z","iopub.execute_input":"2022-11-23T20:06:09.793293Z","iopub.status.idle":"2022-11-23T20:06:10.077278Z","shell.execute_reply.started":"2022-11-23T20:06:09.793257Z","shell.execute_reply":"2022-11-23T20:06:10.076327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir /kaggle/working/saved_models/\ntorch.save(model.state_dict(),\"/kaggle/working/saved_models/transformer_weights.pth\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    score=TextGenerator(20, start_tokens).on_epoch_end(1);\nscore=score.cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2022-11-30T18:42:20.574396Z","iopub.execute_input":"2022-11-30T18:42:20.574748Z","iopub.status.idle":"2022-11-30T18:42:20.615225Z","shell.execute_reply.started":"2022-11-30T18:42:20.574720Z","shell.execute_reply":"2022-11-30T18:42:20.614040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfx = pd.DataFrame(list(np.arange(maxlen)), columns =['Keys'] );\ndfy2=pd.DataFrame(score[0],columns=list(np.arange(maxlen)));\nplt.figure(figsize=(100.0,100.0));\nplt.title(\"Attention scores\");\nplt.xlabel('Keys',size=maxlen);\nplt.ylabel('Queries',size=maxlen);\nplt.plot();\nsns.heatmap(dfy2,fmt=\".3f\",annot=True,linewidths=2,square=True,cmap='twilight');\n","metadata":{"execution":{"iopub.status.busy":"2022-11-30T19:00:08.421194Z","iopub.execute_input":"2022-11-30T19:00:08.422148Z","iopub.status.idle":"2022-11-30T19:00:15.288069Z","shell.execute_reply.started":"2022-11-30T19:00:08.422101Z","shell.execute_reply":"2022-11-30T19:00:15.286149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfx = pd.DataFrame(list(np.arange(maxlen)), columns =['Keys'] );\ndfy2=pd.DataFrame(score[1],columns=list(np.arange(maxlen)));\nplt.figure(figsize=(100.0,100.0));\nplt.title(\"Attention scores\");\nplt.xlabel('Keys',size=maxlen);\nplt.ylabel('Queries',size=maxlen);\nplt.plot();\nsns.heatmap(dfy2,fmt=\".3f\",annot=True,linewidths=2,square=True,cmap='twilight');","metadata":{"execution":{"iopub.status.busy":"2022-11-30T18:42:32.971631Z","iopub.execute_input":"2022-11-30T18:42:32.972808Z","iopub.status.idle":"2022-11-30T18:42:36.841564Z","shell.execute_reply.started":"2022-11-30T18:42:32.972771Z","shell.execute_reply":"2022-11-30T18:42:36.840480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfx = pd.DataFrame(list(np.arange(maxlen)), columns =['Keys'] );\ndfy2=pd.DataFrame(score[2],columns=list(np.arange(maxlen)));\nplt.figure(figsize=(100.0,100.0));\nplt.title(\"Attention scores\");\nplt.xlabel('Keys',size=maxlen);\nplt.ylabel('Queries',size=maxlen);\nplt.plot();\nsns.heatmap(dfy2,fmt=\".3f\",annot=True,linewidths=2,square=True,cmap='twilight');\nplt.savefig(\"head3.png\")","metadata":{"execution":{"iopub.status.busy":"2022-11-30T19:04:14.069420Z","iopub.execute_input":"2022-11-30T19:04:14.069774Z","iopub.status.idle":"2022-11-30T19:04:21.062013Z","shell.execute_reply.started":"2022-11-30T19:04:14.069745Z","shell.execute_reply":"2022-11-30T19:04:21.060883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfx = pd.DataFrame(list(np.arange(maxlen)), columns =['Keys'] );\ndfy2=pd.DataFrame(score[3],columns=list(np.arange(maxlen)));\nplt.figure(figsize=(100.0,100.0));\nplt.title(\"Attention scores\");\nplt.xlabel('Keys',size=maxlen);\nplt.ylabel('Queries',size=maxlen);\nplt.plot();\nsns.heatmap(dfy2,fmt=\".3f\",annot=True,linewidths=2,square=True,cmap='twilight');","metadata":{"execution":{"iopub.status.busy":"2022-11-30T18:42:40.894044Z","iopub.execute_input":"2022-11-30T18:42:40.894415Z","iopub.status.idle":"2022-11-30T18:42:44.965203Z","shell.execute_reply.started":"2022-11-30T18:42:40.894382Z","shell.execute_reply":"2022-11-30T18:42:44.964151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfx = pd.DataFrame(list(np.arange(maxlen)), columns =['Keys'] );\ndfy2=pd.DataFrame(score[4],columns=list(np.arange(maxlen)));\nplt.figure(figsize=(100.0,100.0));\nplt.title(\"Attention scores\");\nplt.xlabel('Keys',size=maxlen);\nplt.ylabel('Queries',size=maxlen);\nplt.plot();\nsns.heatmap(dfy2,fmt=\".3f\",annot=True,linewidths=2,square=True,cmap='twilight');","metadata":{"execution":{"iopub.status.busy":"2022-11-30T18:42:44.967018Z","iopub.execute_input":"2022-11-30T18:42:44.967372Z","iopub.status.idle":"2022-11-30T18:42:49.072015Z","shell.execute_reply.started":"2022-11-30T18:42:44.967340Z","shell.execute_reply":"2022-11-30T18:42:49.071191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfx = pd.DataFrame(list(np.arange(maxlen)), columns =['Keys'] );\ndfy2=pd.DataFrame(score[5],columns=list(np.arange(maxlen)));\nplt.figure(figsize=(100.0,100.0));\nplt.title(\"Attention scores\");\nplt.xlabel('Keys',size=maxlen);\nplt.ylabel('Queries',size=maxlen);\nplt.plot();\nsns.heatmap(dfy2,fmt=\".3f\",annot=True,linewidths=2,square=True,cmap='twilight');\nplt.savefig(\"head6.png\")","metadata":{"execution":{"iopub.status.busy":"2022-11-30T19:01:37.832746Z","iopub.execute_input":"2022-11-30T19:01:37.833162Z","iopub.status.idle":"2022-11-30T19:01:44.915092Z","shell.execute_reply.started":"2022-11-30T19:01:37.833119Z","shell.execute_reply":"2022-11-30T19:01:44.914167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfx = pd.DataFrame(list(np.arange(maxlen)), columns =['Keys'] );\ndfy2=pd.DataFrame(score[6],columns=list(np.arange(maxlen)));\nplt.figure(figsize=(100.0,100.0));\nplt.title(\"Attention scores\");\nplt.xlabel('Keys',size=maxlen);\nplt.ylabel('Queries',size=maxlen);\nplt.plot();\nsns.heatmap(dfy2,fmt=\".3f\",annot=True,linewidths=2,square=True,cmap='twilight');","metadata":{"execution":{"iopub.status.busy":"2022-11-30T18:42:53.065096Z","iopub.execute_input":"2022-11-30T18:42:53.065447Z","iopub.status.idle":"2022-11-30T18:42:56.869721Z","shell.execute_reply.started":"2022-11-30T18:42:53.065414Z","shell.execute_reply":"2022-11-30T18:42:56.868752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfx = pd.DataFrame(list(np.arange(maxlen)), columns =['Keys'] );\ndfy2=pd.DataFrame(score[7],columns=list(np.arange(maxlen)));\nplt.figure(figsize=(100.0,100.0));\nplt.title(\"Attention scores\");\nplt.xlabel('Keys',size=maxlen);\nplt.ylabel('Queries',size=maxlen);\nplt.plot();\nsns.heatmap(dfy2,fmt=\".3f\",annot=True,linewidths=2,square=True,cmap='twilight');","metadata":{"execution":{"iopub.status.busy":"2022-11-30T18:42:56.870940Z","iopub.execute_input":"2022-11-30T18:42:56.871713Z","iopub.status.idle":"2022-11-30T18:43:00.637683Z","shell.execute_reply.started":"2022-11-30T18:42:56.871675Z","shell.execute_reply":"2022-11-30T18:43:00.636597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}