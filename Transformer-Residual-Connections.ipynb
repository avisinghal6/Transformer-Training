{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport random\n\n#replace with pytorch lightning\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n\nfrom sklearn.preprocessing import MinMaxScaler    \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport string","metadata":{"execution":{"iopub.status.busy":"2022-11-30T20:42:09.232082Z","iopub.execute_input":"2022-11-30T20:42:09.233057Z","iopub.status.idle":"2022-11-30T20:42:09.241578Z","shell.execute_reply.started":"2022-11-30T20:42:09.233023Z","shell.execute_reply":"2022-11-30T20:42:09.240218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice = torch.device(device)\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-11-30T20:42:09.243898Z","iopub.execute_input":"2022-11-30T20:42:09.244336Z","iopub.status.idle":"2022-11-30T20:42:09.252992Z","shell.execute_reply.started":"2022-11-30T20:42:09.244293Z","shell.execute_reply":"2022-11-30T20:42:09.251857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n    \"\"\"\n    Mask the upper half of the dot product matrix in self attention.\n    This prevents flow of information from future tokens to current token.\n    1's in the lower triangle, counting from the lower right corner.\n    \"\"\"\n    i=torch.range(1,n_dest)[:,None]\n    j=torch.range(1,n_src)\n    m = i >= j - n_src + n_dest\n    mask=m.bool()\n    return ~mask\n#     mask=torch.reshape(mask, [1, n_dest, n_src])\n#     mult=[batch_size,1,1]\n#     return torch.tile(mask,mult);","metadata":{"execution":{"iopub.status.busy":"2022-11-30T20:42:09.254724Z","iopub.execute_input":"2022-11-30T20:42:09.255435Z","iopub.status.idle":"2022-11-30T20:42:09.263480Z","shell.execute_reply.started":"2022-11-30T20:42:09.255400Z","shell.execute_reply":"2022-11-30T20:42:09.262565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"causal_attention_mask(2,5,5,torch.bool)","metadata":{"execution":{"iopub.status.busy":"2022-11-30T20:42:09.265221Z","iopub.execute_input":"2022-11-30T20:42:09.265620Z","iopub.status.idle":"2022-11-30T20:42:09.279048Z","shell.execute_reply.started":"2022-11-30T20:42:09.265584Z","shell.execute_reply":"2022-11-30T20:42:09.277967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def padding_mask(input):\n    # Create mask which marks the zero padding values in the input by a 1\n#     print(input)\n#     input=torch.tensor(input['train']['input_ids'])\n    mask=torch.eq(input, torch.zeros_like(input))\n\n \n    return mask","metadata":{"execution":{"iopub.status.busy":"2022-11-30T20:42:09.281430Z","iopub.execute_input":"2022-11-30T20:42:09.281717Z","iopub.status.idle":"2022-11-30T20:42:09.287353Z","shell.execute_reply.started":"2022-11-30T20:42:09.281683Z","shell.execute_reply":"2022-11-30T20:42:09.286285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"padding_mask(torch.tensor([[1,2,3,0,0,0],[2,0,0,0,0,0]]))","metadata":{"execution":{"iopub.status.busy":"2022-11-30T20:42:09.288727Z","iopub.execute_input":"2022-11-30T20:42:09.289682Z","iopub.status.idle":"2022-11-30T20:42:09.299679Z","shell.execute_reply.started":"2022-11-30T20:42:09.289631Z","shell.execute_reply":"2022-11-30T20:42:09.298495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(self, embed_dim, num_heads,batch_first,rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.attention = nn.MultiheadAttention(embed_dim,num_heads,batch_first=batch_first)\n\n    def forward(self, inputs,pad_mask):\n        input_shape = inputs.size()\n        batch_size = input_shape[0]\n        seq_len = input_shape[1]\n#         pad_mask=padding_mask(inputs)\n        pad_mask.to(device)\n        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, torch.bool).to(device)\n        attention_output,a = self.attention(inputs, inputs,inputs, key_padding_mask=pad_mask, attn_mask=causal_mask,need_weights=True,average_attn_weights=False)\n        return inputs+attention_output,a","metadata":{"execution":{"iopub.status.busy":"2022-11-30T20:42:09.302081Z","iopub.execute_input":"2022-11-30T20:42:09.302736Z","iopub.status.idle":"2022-11-30T20:42:09.310429Z","shell.execute_reply.started":"2022-11-30T20:42:09.302697Z","shell.execute_reply":"2022-11-30T20:42:09.309469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TokenAndPositionEmbedding(nn.Module):\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        super(TokenAndPositionEmbedding, self).__init__()\n        self.token_emb = nn.Embedding(vocab_size, embed_dim,max_norm=1)\n        self.pos_emb = nn.Embedding(maxlen,embed_dim)\n\n    def forward(self, x):\n        maxlen = x.size()[-1]\n        pad_mask=padding_mask(x)\n        positions = torch.range(start=0, end=maxlen-1, step=1,dtype=torch.int32).to(device)\n        positions = self.pos_emb(positions)\n        x = self.token_emb(x)\n        return (x + positions),pad_mask","metadata":{"execution":{"iopub.status.busy":"2022-11-30T20:42:09.312013Z","iopub.execute_input":"2022-11-30T20:42:09.312616Z","iopub.status.idle":"2022-11-30T20:42:09.320857Z","shell.execute_reply.started":"2022-11-30T20:42:09.312581Z","shell.execute_reply":"2022-11-30T20:42:09.319895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size =50257 #28996  # Only consider the top 20k words\nmaxlen = 60  # Max sequence size\nembed_dim = 128  # Embedding size for each token\nnum_heads = 8  # Number of attention heads\nfeed_forward_dim = 128  # Hidden layer size in feed forward network inside transformer\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n        self.transformer_block1 = TransformerBlock(embed_dim, num_heads,True)\n        self.transformer_block2 = TransformerBlock(embed_dim, num_heads,True)\n        self.MLP1=nn.LazyLinear(feed_forward_dim)\n        self.MLP2=nn.LazyLinear(feed_forward_dim)\n        self.outputs= nn.LazyLinear(vocab_size)\n        \n    def forward(self, x):\n        x,pad_mask = self.embedding_layer(x)\n#         print(x,\"maximum=\",torch.max(x))\n        x,a = self.transformer_block1(x,pad_mask)\n        x=x+self.MLP1(x);\n        x,a = self.transformer_block2(x,pad_mask)\n#         print(x)\n        x=x+self.MLP2(x);\n        x = self.outputs(x)\n        \n        return x,a\n","metadata":{"execution":{"iopub.status.busy":"2022-11-30T20:42:09.396040Z","iopub.execute_input":"2022-11-30T20:42:09.396309Z","iopub.status.idle":"2022-11-30T20:42:09.404660Z","shell.execute_reply.started":"2022-11-30T20:42:09.396277Z","shell.execute_reply":"2022-11-30T20:42:09.403583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from datasets import load_dataset\n\n# dataset = load_dataset(\"wiki_bio\")","metadata":{"execution":{"iopub.status.busy":"2022-11-30T20:42:09.406784Z","iopub.execute_input":"2022-11-30T20:42:09.407911Z","iopub.status.idle":"2022-11-30T20:42:09.413199Z","shell.execute_reply.started":"2022-11-30T20:42:09.407875Z","shell.execute_reply":"2022-11-30T20:42:09.412062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport re\ndirectories = [\n    \"/kaggle/input/aclimdb-v1/aclImdb/train/pos\",\n    \"/kaggle/input/aclimdb-v1/aclImdb/train/neg\",\n    \"/kaggle/input/aclimdb-v1/aclImdb/test/pos\",\n    \"/kaggle/input/aclimdb-v1/aclImdb/test/neg\",\n]\n\nfrom datasets import load_dataset\nfilenames = []\nfor dir in directories:\n    for f in os.listdir(dir):\n        filenames.append(os.path.join(dir, f))\n\ndataset = load_dataset(\"text\", data_files=filenames)\n\ndef processing(s):\n  s['text']=s['text'].lower()\n  s['text']=re.sub(\"<br />\", \" \", s['text'])\n  s['text']=re.sub(f\"([{string.punctuation}])\", r\" \\1\", s['text'])\n  return s\n\ndataset=dataset.map(processing)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-11-30T20:42:09.414595Z","iopub.execute_input":"2022-11-30T20:42:09.415810Z","iopub.status.idle":"2022-11-30T20:47:31.075450Z","shell.execute_reply.started":"2022-11-30T20:42:09.415763Z","shell.execute_reply":"2022-11-30T20:47:31.074446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers import AutoTokenizer\n# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\nfrom transformers import GPT2Tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")","metadata":{"execution":{"iopub.status.busy":"2022-11-30T20:47:31.076960Z","iopub.execute_input":"2022-11-30T20:47:31.077310Z","iopub.status.idle":"2022-11-30T20:47:42.045240Z","shell.execute_reply.started":"2022-11-30T20:47:31.077273Z","shell.execute_reply":"2022-11-30T20:47:42.044202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = dataset.map(lambda dataset: tokenizer(dataset[\"text\"],truncation=True, max_length=maxlen))","metadata":{"execution":{"iopub.status.busy":"2022-11-30T20:47:42.048208Z","iopub.execute_input":"2022-11-30T20:47:42.048678Z","iopub.status.idle":"2022-11-30T20:49:39.017768Z","shell.execute_reply.started":"2022-11-30T20:47:42.048625Z","shell.execute_reply":"2022-11-30T20:49:39.016747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def padding(s):\n    if len(s['input_ids'])<maxlen :\n        s['input_ids']=s['input_ids']+[0]*(maxlen-len(s['input_ids']))\n    return s\n                                           \ndataset=dataset.map(padding)\n","metadata":{"execution":{"iopub.status.busy":"2022-11-30T20:49:39.019410Z","iopub.execute_input":"2022-11-30T20:49:39.019783Z","iopub.status.idle":"2022-11-30T20:49:46.950385Z","shell.execute_reply.started":"2022-11-30T20:49:39.019748Z","shell.execute_reply":"2022-11-30T20:49:46.949393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2022-11-30T20:49:46.951743Z","iopub.execute_input":"2022-11-30T20:49:46.952090Z","iopub.status.idle":"2022-11-30T20:49:46.959764Z","shell.execute_reply.started":"2022-11-30T20:49:46.952051Z","shell.execute_reply":"2022-11-30T20:49:46.958709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=Model()\n# model.load_state_dict(torch.load(\"/kaggle/input/weights3/transformer_weights-3.pth\")) \nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-11-30T20:49:46.961245Z","iopub.execute_input":"2022-11-30T20:49:46.961603Z","iopub.status.idle":"2022-11-30T20:49:47.276880Z","shell.execute_reply.started":"2022-11-30T20:49:46.961568Z","shell.execute_reply":"2022-11-30T20:49:47.275690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<!-- # model= Model()\n# model.to(device) -->","metadata":{"execution":{"iopub.status.busy":"2022-11-23T19:37:34.977324Z","iopub.execute_input":"2022-11-23T19:37:34.977741Z","iopub.status.idle":"2022-11-23T19:37:35.372676Z","shell.execute_reply.started":"2022-11-23T19:37:34.977705Z","shell.execute_reply":"2022-11-23T19:37:35.371685Z"}}},{"cell_type":"code","source":"class TextGenerator(nn.Module):\n    \"\"\"A callback to generate text from a trained model.\n    1. Feed some starting prompt to the model\n    2. Predict probabilities for the next token\n    3. Sample the next token and add it to the next input\n\n    Arguments:\n        max_tokens: Integer, the number of tokens to be generated after prompt.\n        start_tokens: List of integers, the token indices for the starting prompt.\n        index_to_word: List of strings, obtained from the TextVectorization layer.\n        top_k: Integer, sample from the `top_k` token predictions.\n        print_every: Integer, print after this many epochs.\n    \"\"\"\n\n    def __init__(\n        self, max_tokens, start_tokens, top_k=10, print_every=1\n    ):\n        self.max_tokens = max_tokens\n        self.start_tokens = start_tokens\n#         self.index_to_word = index_to_word\n        self.print_every = print_every\n        self.k = top_k\n\n    def sample_from(self, logits):\n        logits, indices = torch.topk(logits, k=self.k, sorted=True)\n        logits=logits.cpu()\n        indices=indices.cpu()\n        indices = np.asarray(indices).astype(\"int32\")\n       \n        softmax=nn.Softmax(dim=0)\n        preds = softmax(logits)\n        preds = np.asarray(preds).astype(\"float32\")\n#         return np.random.choice(indices, p=preds) THIS IS THE CORRECT CODE, BUT HAD TO COMMENT IT AS\n#.        PROBABILITIES HAVE NAN AND I HAD TO VERIFY PIPELINE, BELOW LINE WILL BE REMOVED ONCE NAN ISSUE \n#.        IS RESOLVED\n        return np.random.choice(indices, p=preds)\n#         return np.random.choice(5, 1, p=[0.1, 0, 0.3, 0.6, 0])\n\n    def detokenize(self, number):\n        return tokenizer.decode(number)\n\n    def on_epoch_end(self, epoch, logs=None):\n        start_tokens = [_ for _ in self.start_tokens]\n        if (epoch + 1) % self.print_every != 0:\n            return\n        num_tokens_generated = 0\n        tokens_generated = []\n        attention_scores=[]\n        while num_tokens_generated <= self.max_tokens:\n            pad_len = maxlen - len(start_tokens)\n            sample_index = len(start_tokens) - 1\n            if pad_len < 0:\n                data = start_tokens[:maxlen]\n                sample_index = maxlen - 1\n            elif pad_len > 0:\n                data = start_tokens + [0] * pad_len\n            else:\n                data = start_tokens\n                \n            data = torch.Tensor(np.array([data])).type(torch.int32).to(device)\n            \n            y,attention_scores = model(data)\n            sample_token = self.sample_from(y[0][sample_index])\n            tokens_generated.append(sample_token)\n            start_tokens.append(sample_token)\n            num_tokens_generated = len(tokens_generated)\n        txt = \" \".join(\n            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n        )\n        print(f\"generated text:\\n{txt}\\n\")\n        return attention_scores, txt\n\n# Tokenize starting prompt\n# word_to_index = {}\n# for index, word in enumerate(vocab):\n#     word_to_index[word] = index\nstart_prompt = \"Mr and Mrs Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you'd expect to be involved in anything strange or mysterious, because they just didn't hold with such nonsense. Mr\"\n\n# start_prompt = \"Brilliant over-acting by Lesley Ann Warren. Best dramatic hobo lady I have ever seen, and love scenes in clothes warehouse are second to none. The dramatic hobo\"\nstart_tokens=tokenizer(start_prompt)['input_ids']\n# start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\nnum_tokens_generated = 40\n# text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)","metadata":{"execution":{"iopub.status.busy":"2022-11-30T20:49:47.278467Z","iopub.execute_input":"2022-11-30T20:49:47.279243Z","iopub.status.idle":"2022-11-30T20:49:47.298745Z","shell.execute_reply.started":"2022-11-30T20:49:47.279201Z","shell.execute_reply":"2022-11-30T20:49:47.297753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\ntrain_dataset= Dataset.from_dict({\"id\": dataset['train']['input_ids']})\ntrain_dataset = train_dataset.with_format(\"torch\")","metadata":{"execution":{"iopub.status.busy":"2022-11-30T20:49:47.300878Z","iopub.execute_input":"2022-11-30T20:49:47.301311Z","iopub.status.idle":"2022-11-30T20:49:49.074983Z","shell.execute_reply.started":"2022-11-30T20:49:47.301265Z","shell.execute_reply":"2022-11-30T20:49:49.073958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count=0\nTEST=[]\ntrain_loader=DataLoader(train_dataset,batch_size=50,shuffle=True)\nfor i in train_loader:\n    TEST.append(random.choice(i['id']))\n    if count>100:\n        break\n    count+=1","metadata":{"execution":{"iopub.status.busy":"2022-11-30T20:49:49.076256Z","iopub.execute_input":"2022-11-30T20:49:49.076590Z","iopub.status.idle":"2022-11-30T20:49:49.977724Z","shell.execute_reply.started":"2022-11-30T20:49:49.076557Z","shell.execute_reply":"2022-11-30T20:49:49.976511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TEST=torch.stack(TEST,0) \n# print(TEST[:,:])","metadata":{"execution":{"iopub.status.busy":"2022-11-30T20:49:49.979472Z","iopub.execute_input":"2022-11-30T20:49:49.979861Z","iopub.status.idle":"2022-11-30T20:49:49.985378Z","shell.execute_reply.started":"2022-11-30T20:49:49.979822Z","shell.execute_reply":"2022-11-30T20:49:49.983967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader=DataLoader(train_dataset,batch_size=50,shuffle=True)\noptim=torch.optim.AdamW(model.parameters(),lr=1e-4)\nloss_fn=torch.nn.CrossEntropyLoss()\ncount=0\nloss_stats = {\n    'test': [],\n}\nfor epoch in tqdm(range(20)):\n    for batch in tqdm(train_loader):\n        optim.zero_grad()\n#         print(batch['id'][:,:-1])\n        input_ids=batch['id'][:,:-1].to(device)\n#         print(input_ids.shape())\n        labels=batch['id'][:,1:].to(device)\n        outputs,attention_scores=model.forward(input_ids)\n        labels=nn.functional.one_hot(labels,num_classes=vocab_size).type(torch.float)\n        loss=loss_fn(outputs,labels)\n        loss.backward()\n        optim.step()\n    \n    with torch.no_grad():\n        TextGenerator(40, start_tokens).on_epoch_end(epoch);\n        test_input=TEST[:,:-1].to(device)\n        test_output=TEST[:,1:].to(device)\n        outputs,attention_scores=model.forward(test_input)\n        labels=nn.functional.one_hot(test_output,num_classes=vocab_size).type(torch.float)\n        loss=loss_fn(outputs,labels).cpu().item()\n        loss_stats['test'].append(loss)\n        ","metadata":{"execution":{"iopub.status.busy":"2022-11-30T20:49:49.987165Z","iopub.execute_input":"2022-11-30T20:49:49.987475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loss_df = pd.DataFrame.from_dict(loss_stats).reset_index().melt(id_vars=['index']).rename(columns={\"index\":\"epochs\"})\n# Plot the dataframes\nfig,axes = plt.subplots(nrows=1, ncols=1, figsize=(20,7))\nsns.lineplot(data=test_loss_df, x = \"epochs\", y=\"value\", hue=\"variable\",  ax=axes).set_title('TestLoss')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir /kaggle/working/saved_models/\ntorch.save(model.state_dict(),\"/kaggle/working/saved_models/transformer_weights.pth\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    score, txt=TextGenerator(6, start_tokens).on_epoch_end(1);\nscore=score.cpu().numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(score.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(start_tokens)\nlen(start_prompt.split())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"txt.split()[32]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfx = pd.DataFrame(list(np.arange(maxlen)), columns =['Keys'] );\ndfy2=pd.DataFrame(score[0][0],columns=list(np.arange(maxlen)));\nplt.figure(figsize=(100.0,100.0));\nplt.title(\"Attention scores\");\nplt.xlabel('Keys',size=maxlen);\nplt.ylabel('Queries',size=maxlen);\nplt.plot();\nsns.heatmap(dfy2,fmt=\".3f\",annot=True,linewidths=2,square=True,cmap='twilight');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfx = pd.DataFrame(list(np.arange(maxlen)), columns =['Keys'] );\ndfy2=pd.DataFrame(score[0][1],columns=list(np.arange(maxlen)));\nplt.figure(figsize=(100.0,100.0));\nplt.title(\"Attention scores\");\nplt.xlabel('Keys',size=maxlen);\nplt.ylabel('Queries',size=maxlen);\nplt.plot();\nsns.heatmap(dfy2,fmt=\".3f\",annot=True,linewidths=2,square=True,cmap='twilight');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfx = pd.DataFrame(list(np.arange(maxlen)), columns =['Keys'] );\ndfy2=pd.DataFrame(score[0][2],columns=list(np.arange(maxlen)));\nplt.figure(figsize=(100.0,100.0));\nplt.title(\"Attention scores\");\nplt.xlabel('Keys',size=maxlen);\nplt.ylabel('Queries',size=maxlen);\nplt.plot();\nsns.heatmap(dfy2,fmt=\".3f\",annot=True,linewidths=2,square=True,cmap='twilight');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfx = pd.DataFrame(list(np.arange(maxlen)), columns =['Keys'] );\ndfy2=pd.DataFrame(score[0][3],columns=list(np.arange(maxlen)));\nplt.figure(figsize=(100.0,100.0));\nplt.title(\"Attention scores\");\nplt.xlabel('Keys',size=maxlen);\nplt.ylabel('Queries',size=maxlen);\nplt.plot();\nsns.heatmap(dfy2,fmt=\".3f\",annot=True,linewidths=2,square=True,cmap='twilight');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfx = pd.DataFrame(list(np.arange(maxlen)), columns =['Keys'] );\ndfy2=pd.DataFrame(score[0][4],columns=list(np.arange(maxlen)));\nplt.figure(figsize=(100.0,100.0));\nplt.title(\"Attention scores\");\nplt.xlabel('Keys',size=maxlen);\nplt.ylabel('Queries',size=maxlen);\nplt.plot();\nsns.heatmap(dfy2,fmt=\".3f\",annot=True,linewidths=2,square=True,cmap='twilight');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfx = pd.DataFrame(list(np.arange(maxlen)), columns =['Keys'] );\ndfy2=pd.DataFrame(score[0][5],columns=list(np.arange(maxlen)));\nplt.figure(figsize=(100.0,100.0));\nplt.title(\"Attention scores\");\nplt.xlabel('Keys',size=maxlen);\nplt.ylabel('Queries',size=maxlen);\nplt.plot();\nsns.heatmap(dfy2,fmt=\".3f\",annot=True,linewidths=2,square=True,cmap='twilight');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfx = pd.DataFrame(list(np.arange(maxlen)), columns =['Keys'] );\ndfy2=pd.DataFrame(score[0][6],columns=list(np.arange(maxlen)));\nplt.figure(figsize=(100.0,100.0));\nplt.title(\"Attention scores\");\nplt.xlabel('Keys',size=maxlen);\nplt.ylabel('Queries',size=maxlen);\nplt.plot();\nsns.heatmap(dfy2,fmt=\".3f\",annot=True,linewidths=2,square=True,cmap='twilight');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfx = pd.DataFrame(list(np.arange(maxlen)), columns =['Keys'] );\ndfy2=pd.DataFrame(score[0][7],columns=list(np.arange(maxlen)));\nplt.figure(figsize=(100.0,100.0));\nplt.title(\"Attention scores\");\nplt.xlabel('Keys',size=maxlen);\nplt.ylabel('Queries',size=maxlen);\nplt.plot();\nsns.heatmap(dfy2,fmt=\".3f\",annot=True,linewidths=2,square=True,cmap='twilight');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}