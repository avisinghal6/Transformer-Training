# Transformer-Training

Transformer-2-Layer-Attention.ipynb contains the code for 2 layer attention model using iMDB.
Transformer-2-Layer-MLP.ipynb contains the code for 2 layer attention model +MLP layers using iMDB.
Transformer-BERT-Tokenizer.ipynb contains the code for 1 Layer attention but uses BERT tokenizer, the rest of the files use GPT2 tokenizer using iMDB.

Transformer-sweep.ipynb contains the code for 1 layer attention model sweep using wandb using iMDB.
WIKITEXT-600K-Transformer.ipynb contains the code for 1 layer attention model and the dataset is WIKITEXT, we used 600K datapoints.

WIKITEXT-Full-data-Transformer.ipynb contains the code for 1 layer attention model and the dataset is WIKITEXT, we used full datap.

We ran all the experiments using Kaggle.
